\chapter{Future Work}
\label{ch:future}

This thesis demonstrates a principled approach to optimizing dynamic neural networks
    that generalizes the performance enjoyed by static neural networks to a greater
    number of networks.
There are still open questions on how to optimize and compile
    generic tensor programs to arbitrary hardware targets.
TVM is continuing to evolve rapidly
    with many changes in the past year not detailed
    in this thesis.
In particular completing the IR unification, improved
    dynamic model support, training support, automatic
    scheduling for new hardware targets and much more.
The remainder of this section touches on some of the
    challenges that lie in front of this work.

\subsection{Unified IR}

TVM has been working on unifying its multiple levels of
    IR into a family of dialects which can invoke functions
    at different level of abstractions.
The start of this work is complete but using the
    new unified world to meaningfully improve optimizations
    is something that has not yet been demonstrated.
For example one could lower an entire Relay program
    to a single GPU kernel using the new machinery,
    but no one has attempted such an optimization yet.

\subsection{Going further with Dynamic Models}

There are still problems which are unsolved in Relay's
    current iteration.
There are analyses that can be further improved such as
    recover further static shape information for optimization
    using a more precise analysis.
Or features not yet supported such as dynamically ranked tensors.
There are still interesting challenges to solve here as
    models evolve and demand more from compilers.s

\subsection{Training Support}

A team at OctoML and AMD have begun work on using TVM
    to accelerate training by building a framework
    that lowers all computation to Relay which
    can make use of all the techniques described
    in this thesis.
Although the work just began we were able to
    get models working on AMD GPUS with no custom
    code needed a feat most frameworks have
    failed to achieve.
There significant work to make this a state
    of the art competitive approach with existing
    frameworks but an interesting area going forward
    as TVM allows training to be deployed to any
    supported device.

\subsection{Automatic Scheduling}

A final promising area being explored at OctoML is
    automatic scheduling of tensor programs, including
    tensorization.
Originally in TVM a user must provide the network,
    operator definitions, and schedules.
AutoTVM introduced the ability to define a template
    schedule where some parameters such as tiling
    factor, or loop split can be learned using ML.
Ansor then introduced ability to learn both the
    template and the parameters using ML guided
    search.
The finally vision of these systems is to allow
    a Relay program to be lowered to TVM's TIR
    and then scheduled automatically allowing
    users to interpolate between manually
    knowledge and fully automatically.
This would enable users to leverage the
    expressivity of Relay, then lower to TIR,
    and finally rewrite the code to achieve
    near optimal performance using ML guided
    search.


\subsection{Conclusion}
These directions represent potentially years more work, and will be hopefully
    realized by collaborators PhD theses, future publications, the open source
    community and development teams at OctoML.
The work done in this this presents a compelling foundation on which to build
    these future efforts.
