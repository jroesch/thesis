\section{Type System}
\label{sec:type_system}

Relay's type system is essential
  to optimizations.
Typing guarantees both well-formedness of the program
  and provides crucial tensor shape information to perform allocation,
  check correctness, and facilitate loop optimizations.
Computation graph IRs rely on typing in the form of
  datatype and shape inference.
Datatype and shape inference is the process of computing the
  concrete datatypes (e.g., \verb|float32|, \verb|int32|) and shapes (e.g., $(10, 5)$, $(100, 1, 32)$) of all
  tensors in a computation graph.
Deep learning frameworks and compilers use static shape information
  to perform allocation, check correctness, and facilitate optimization.
Precise static shape information is also valuable for traditional loop
  optimizations, data layout transformations, tensorization, and
  optimizations that are necessary to map to hardware accelerators' unique ISAs.
In computation graph IRs, only numeric data types
  and shapes are tracked for each operator.
Symbolic shapes (i.e., shape polymorphism) are only handled
  dynamically, inhibiting certain types of optimizations.
Shape inference is usually formulated as a simple analysis over the dataflow graph that
  propagates shape information.
Shape inference looks remarkably similar to type inference.
Unlike type inference, though, shape inference is separate from the type system and
  does not provide types for functions or data structures.
Handling shape inference at compile time is desirable, because it allows optimizations to take
  advantage of this information even though certain shapes may be symbolic. Can shape information be encoded in static types?
If we type Relay as simply typed lambda calculus,
  we gain a simple system, but one that can not represent polymorphism,
  and lacks shape information.
Even with the addition of polymorphism, there is no representation of static
  shape information.
  It is possible to model arbitrarily complex static properties, such
  as shape information, with a dependent type theory~\citep{selsam_certigrad}, but such
  a design incurs significant user complexity.
Adopting a well known type system allows the application of
  classic techniques, but standard type systems do not
  provide a solution for tracking static shape information.
By incorporating shape analysis into a broader type system,
  Relay's type system balances the desire for static tensor shapes
  with usability.
In this subsection we describe how to extend a polymorphic type system with shape
  information and type inference with shape inference.

\begin{figure}
  \begin{footnotesize}
    \judgbox{\typecheck{\typeCtx}{\varCtx}{\expr}{\type}}{Expression $\expr$ has type $\type$ in type context $\typeCtx$ and variable context $\varCtx$.}
    \begin{inference}
    \jmpInfer{Relation-T}
       {\Delta, T_1 : \texttt{Type}, \ldots, T_n : \texttt{Type} \vdash (Rel(T_1, T_2, \ldots, T_n) \in \{ \top, \bot \}) }
       {\typecheck{\typeCtx}{\varCtx}{Rel}{\texttt{Relation}}}

    \jmpInfer{Type-Func-Def}
      {\forall{i \in [1, r]} \, \Delta; \Gamma \vdash R_i(T_1, \ldots, T_n, O) \\
       \Delta; \Gamma, a_1 : T_1, \ldots, a_n : T_n, \\
       f : \kwd{fn}( T_1, \ldots, T_n) \rightarrow O \kwd{ where } R_1, \ldots, R_r \vdash body : O}
        {\Delta; \Gamma \vdash \kwd{def @} f\kwd{(}a_1\kwd{:} T_1\kwd{,} \ldots
        a_n\kwd{:} T_n\kwd{)} \rightarrow O \kwd{ where } R_1, \ldots, R_r \kwd{ \{ } body \kwd{ \}} : \\
        \kwd{fn}(T_1, \ldots, T_n) \rightarrow O \kwd{ where } R_1, \ldots, R_r }

    \jmpInfer{Type-Call}
       {\Delta; \Gamma \vdash f : \kwd{fn} (T_1 \kwd{,} \ldots \kwd{,} T_n) \rightarrow O
         \kwd{ where } R_1, \ldots, R_r
         \\ \Delta; \Gamma \vdash a_1 : T_1, \ldots, a_n : T_n
         \\ \forall{i \in [1, r]} \, \Delta; \Gamma \vdash R_i(T_1, \ldots, T_n, O)}
       {\typecheck{\typeCtx}{\varCtx}{f(a_1, \ldots, a_n)}{O}}
    \end{inference}
  \end{footnotesize}
  \caption{Examples of Relay's typing inference rules, namely the rules for function definitions and function calls,
    where $\Delta$ is the environment for types and $\Gamma$ is the environment for variables. These demonstrate
    that type relations must hold at each call site.}
  \label{fig:partial-inference-rules}
\end{figure}


\subsection{Tensor Types}

The primitive value in Relay is a tensor, which has
  a shape and a base type (\verb|tensor type| in Figure \ref{fig:short_bnf}).
Base types describe the elements of tensors by tracking
  the bit width,
  the number of lanes (for utilizing vectorized intrinsics),
  and whether the type is floating point or integral.
To ensure Relay can offload tensor computation to devices
  with greatly varying architectures,
  Relay tensors may only contain base types,
  preventing, for example, tensors of closures.
The shape of a tensor is a tuple of integers describing the tensor's dimensions.
A dimension may be a variable or arithmetic expression that indicates how the
  output shape of an operator depends on those of its inputs.
Functions may be polymorphic over shapes, which results
  in shape constraints that must be solved during type inference.
Sec.~\ref{sec:inference} describes the process.

In the context of dynamic models, many data shapes can only be determined at runtime.
Therefore, the previous assumption of static data shapes does not hold.
In order to support dynamic data shapes, Relay VM introduces a special dimension called
  Any~to represent statically unknown dimensions.
For example, we can represent a tensor type as \texttt{Tensor[(1, 10, Any), float32]},
  where the size of the third dimension in this tensor is unknown while the other
  two dimensions have concrete values.
This concept has been introduced in other frameworks.
We do not support tensor types with dynamic ranks given the relatively limited use cases and optimization opportunities.

\subsection{Operators and Type Relations}
A difference between general purpose programming models and those tailored to deep learning
  is the use of operators as the primitive unit of computation.
Relay's use of opaque operators enables backends to choose different
  lowering strategies based on the hardware target.
Relay's operator set is extensible, meaning that users may add new operations.
Supporting common or user-defined tensor operators requires
  a type system that can adapt to complex shape
  relationships between input and output types
(e.g., elementwise operators with broadcasting semantics).
For example some operators have result types which can be
  written as a function of the input types.
For example, we use a relation that describes the
  broadcasting rule for all element-wise operations.
Unfortunately some are not only functions,
  but also relations that specify constraints between input and output shapes.
To handle the constraints between operators' argument shapes,
  Relay's type system introduces a concept of type relations.
A type relation is implemented as a function in the
  meta-language and represents a symbolic relationship between
  the input and output types.
When developers add a new operator to Relay,
  they may constrain its type with an existing relation or add their own.
Function types (including those of operators) may include
  one or more type relations over an arbitrary subset
  of the argument types and the return type.
The type checker enforces that these relationships hold at the call site.
These relations may be viewed as a verification condition induced at a
  function call site, where the formula is a conjunction of the relations.
For example, primitive operators are assigned types that are universally
  quantified over both the input and output types.
We can then use a type relation to encode a constraint that must hold later
  when type checking observes specific input and output types.
We first describe these relation in depth, then discuss how they
  are used in type inference.

\subsection{Operator Type Relation}
A type relation describes the relationship between
  the types of operator inputs and outputs.
The type system of TVM's Relay IR relies on these
  type relations to infer and bidirectionally
  propagate type and shape relationships between
  inputs and outputs of operators across the whole program.
For example, the type relation for broadcasting operators (e.g. \texttt{broadcast\_add}) performs the shape
  calculation following the NumPy broadcasting rules\footnote{\url{https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html}}.

The type relation must be generalized to properly handle dynamic shapes. For example, a program which grows a tensor on
each loop iteration (a case existing in the decoder of many NLP models) is both impossible to type and compile without proper type system support.
With the introduction of Any, we are able to improve the existing type relations to support dynamic models.

There are two cases that must be handled after we introduce Any~to the type relations.
First, operators such as {\tt arange}\footnote{{\tt arange} generates a range of values in a (start, stop, step) interval the arrays output size is a function of input arguments.} and {\tt unique}\footnote{{\tt unique} selects the unique elements of a tensor.} have dynamic output shapes depending on the input data, which will be described in Any.
We can now describe the type relation using the enhanced type relation functions with Any but not before.
For example, type relation of {\tt arange} can be expressed as follows
\begin{minted}{rust}
arange: fn(start: fp32, stop: fp32, step: fp32) -> ([Any], fp32)
\end{minted}

Second, when input shapes of a type relation contain an \verb|Any|~dimension,
  the type relation needs to propagate Any~correctly
  to the output types and relax typing constraints
  that hold in the static cases when necessary.
For example, type relation for {\tt broadcasting} operators
  determines the compatibility between corresponding
  dimensions from both inputs if they are equal or
  one of them is 1 according to the Numpy broadcasting rule.
For example, the rules for broadcast type relation given
  the matching dimension from two inputs when having
    Any~are defined as follows:
\begin{align*}
  \textrm{broadcast\_rel}(Any, 1) &\rightarrow Any \\
  \textrm{broadcast\_rel}(Any, d) &\rightarrow d ~~~~~~(d > 1) \\
  \textrm{broadcast\_rel}(Any, Any) &\rightarrow Any.
\end{align*}
Note that due the presence of dynamic shapes these type
  relation rules can no longer rule out all type errors at compile-time.
For example, for the second rule shown above, when
  Any~is neither 1 nor $d$ at runtime, it then violates
  the broadcast type constraints.
To address this, we take the gradual typing~\citep{gradualtyping}
  approach and leave certain type checking at runtime after
  Any~is instantiated by a concrete value (see \autoref{sec:compilation:shape-func}
  for more details).
One could eliminate these errors using a more advanced type system, but at increased complexity.

\section{Type Inference}
\label{sec:inference}

The most interesting parts of the type system
are where shape computation occurs.
We highlight a few examples of Relay's inference
rules in Fig.~\ref{fig:partial-inference-rules};
the full typing rules can be found in the appendix.
In this subsection we focus on design decisions behind Relay's type system
and the implementation of type inference.
To incorporate type relations into Relay's type system, we enrich
a Hindley-Milner type inference algorithm with
a constraint solver for type relations.
Relay's inference algorithm has three steps: first it
performs a pass over the AST generating types (potentially involving type variables)
as well as populating the set of relations,
then it solves the incurred constraints,
and finally it assigns types to each expression in the AST.
When the type inference algorithm visits a function call site, the function's type relations are
instantiated to the types at the call site and added to a queue of relations waiting to be
solved.
The relationships between the call's type variables and its relations are are added to a
bipartite undirected dependency graph where the two disjoint sets are type variables and type relations.
Traditional unification constraints are represented using a modified union-find structure that
integrates with the dependency graph.

Once the queue is populated, the algorithm will dequeue a relation and attempt to solve it.
There are two cases when solving a type relation:
\begin{enumerate}
\item If all the relation's type variables
are concrete, we call the type relation function. If that function returns true, the
constraint is discharged. Otherwise, type checking fails.
\item If any type is fully or partially symbolic, the
  algorithm will propagate
  existing concrete type information via unification.
All relations affected by new assignments to type
  variables (as determined by the dependency graph)
  are moved to the beginning of the queue.
If the current type relation is now completely solved, we
discard it to avoid unnecessarily visiting it again.
\end{enumerate}

Our fine-grained dependence graph provides the transitive dependencies
between relations and unification variables.
The use of fine-grained dependencies enables our algorithm to
only retry a minimal number of relations when we
learn a new variable assignment.
We run this to fixpoint or until the queue is empty.
If the queue is not empty and no progress is made between iterations,
then at least one variable is under constrained and inference fails.
Note that a type relation's implementation can
compromise type soundness, as they are axiomatic descriptions
of operations implemented outside of Relay.
Luckily, in practice, the number of type relations needed to express most of Relay's
operators is relatively small, and their implementations are generally straightforward
and amenable to exhaustive testing.
The above sections discuss the core of Relay, a full-length paper under submission\citep{roesch2019relay}
details its design and implementation.

To incorporate type relations into Relay's type system, we enrich
a Hindley-Milner-style type inference algorithm with
a constraint solver.
Relay's inference algorithm has three steps: first, it
performs a pass over the AST, generating types and a set of relations,
then it solves the incurred constraints,
and finally annotates each sub-expression with its inferred type.

% a queue of relations. dequeue a relation and attempt to solve it by invoking it's opaque
% type-relation function.
% 1. all type variables concrete & relationship holds => solved. discard relation
% 2. all type variables concrete & relationship doesn't hold => type error
% 3. types are partially symbolic => propagate unif. constraints to its inputs and output. if
% all type variables are solved/assigned/concrete, discard relation. otherwise add it and the
% relations that depend on the solved variables to queue. those that are already on the queue should
% be reprioritized

When the type inference algorithm visits a function call site, the function's type relations are
instantiated with the concrete argument types at the call site.
Each instantiated relation is added to the queue of relations to solve.
The relationship between a call's type variables and relations is added as an edge to
a bipartite dependency graph where the two disjoint sets are type variables and type relations.
Traditional unification constraints are represented using a modified union-find structure that
integrates with this dependency graph.

Once the queue is populated, the algorithm will dequeue a relation and attempt to solve it.
There are two cases when solving a type relation:
\begin{enumerate}
\item If all the relation's type variables
are concrete, we the relation function. If that function returns true, the
constraint is discharged. Otherwise, type checking fails.
\item If any type is fully or partially symbolic, the
  algorithm will propagate
  existing concrete type information via unification.
All relations affected by new assignments to type
  variables (as determined by the dependency graph)
  are moved to the beginning of the queue.
If the current type relation is now completely solved, we
discard it to avoid unnecessarily visiting it again.
\end{enumerate}

We run this to fixpoint or until the queue is empty.
If the queue is non-empty and no progress is made between iterations,
then at least one variable is underconstrained and inference fails.
Note that a type relation's implementation can
compromise type soundness, as they are axiomatic descriptions
of operations implemented outside of Relay.
In practice, the number of type relations needed to express Relay's
operators is small, and their implementations are straightforward
and amenable to exhaustive testing.

To incorporate type relations into Relay's type system, we enrich
  a Hindley-Milner-style type inference algorithm with
  a constraint solver.
Relay's inference algorithm has three steps: first, it
  performs a pass over the AST, generating types and a set of relations,
  then it solves the incurred constraints,
  and finally annotates each sub-expression with its inferred type.

% a queue of relations. dequeue a relation and attempt to solve it by invoking it's opaque
% type-relation function.
% 1. all type variables concrete & relationship holds => solved. discard relation
% 2. all type variables concrete & relationship doesn't hold => type error
% 3. types are partially symbolic => propagate unif. constraints to its inputs and output. if
% all type variables are solved/assigned/concrete, discard relation. otherwise add it and the
% relations that depend on the solved variables to queue. those that are already on the queue should
% be reprioritized

When the type inference algorithm visits a function call site, the function's type relations are
  instantiated with the concrete argument types at the call site.
Each instantiated relation is added to the queue of relations to solve.
The relationship between a call's type variables and relations is added as an edge to
  a bipartite dependency graph where the two disjoint sets are type variables and type relations.
Traditional unification constraints are represented using a modified union-find structure that
  integrates with this dependency graph.

Once the queue is populated, the algorithm will dequeue a relation and attempt to solve it.
There are two cases when solving a type relation:
\begin{enumerate}
  \item If all the relation's type variables
  are concrete, we the relation function. If that function returns true, the
  constraint is discharged. Otherwise, type checking fails.
  \item If any type is fully or partially symbolic, the
    algorithm will propagate
    existing concrete type information via unification.
  All relations affected by new assignments to type
    variables (as determined by the dependency graph)
    are moved to the beginning of the queue.
  If the current type relation is now completely solved, we
  discard it to avoid unnecessarily visiting it again.
\end{enumerate}

We run this to fix-point or until the queue is empty.
If the queue is non-empty and no progress is made between iterations,
  then at least one variable is unconstrained and inference fails.
Note that a type relation's implementation can
  compromise type soundness, as they are axiomatic descriptions
  of operations implemented outside of Relay.
In practice, the number of type relations needed to express Relay's
  operators is small, and their implementations are straightforward
  and amenable to exhaustive testing.

One caveat of the Any~dimension is that unknown dimensions will
  propagate during type inference, reducing chances for shape specialization.
For example, if we use an operator such as {\tt arange} to produce a
  tensor with dynamic shape (i.e., \texttt{Tensor[(Any,), float32]})
  and later {\tt broadcast\_add} to a tensor with static shape (i.e., \texttt{Tensor[(5, 1), float32]}),
  the output shape will also contain an Any~dimension (i.e., \texttt{Tensor[(5, Any), float32])}.

To limit the contamination of Any, we further introduce {\em sub-shaping}
  to improve the precision of types computed by type-inference.
Much like sub-typing used in popular programming languages \citep{LiskovTPLS1994,AmadioAmadioTPLS1993},
  our extension enables values with more specific shape information to be passed in contexts which require less specific shapes.
Further, we perform extra analysis that repeatedly replace one Any~dimension by a symbolic variable followed by
  a type inference pass to identify if other Any~dimensions share the same symbolic expression.
%on each Any~dimension to detect if two Any~dimensions point to an identically sized dimension.
This information is passed to downstream compilation
  and can reduce the search space during the symbolic codegen.
We can use this analysis in the downstream compilation
  to generate shape-specialized code during codegen.
We treat concrete dimension and symbolic dimension as a sub-shape of Any.

\subsection{Shape Function}
\label{sec:type_systemTech:shape-func}
Existing deep learning compilers only deal with static shapes,
  enabling all shape computation to occur at compile-time.
Therefore, it is easy to allocate memory for the tensors
  using static shape information to compute the precise amount of storage needed for each tensor.
However, introducing Any~invalidates this pre-allocation
  mechanism as the tensors may now contain dynamic dimensions.
The introduction of Any~dimension invalidates the pre-allocation
  mechanism adopted in the existing deep learning compiler.
Instead, we now have to track the amount of memory required to be allocated in parallel to computing.
Furthermore, static type checking cannot eliminate all
  type errors at compile-time due to dynamic tensor shapes.
Consequently, we define a {\em shape function} to compute the output shape
  for storage allocation and verify the type relation in accord with the semantics of every operators.
The shape function is similar in structure to the type relations described in
  \ref{sec:type_system} but are present at runtime instead of compile-time.
The shape function enables compiling and embedding the computation of output
  shapes into the program.
% TODO: raises question of code duplication vs existing type rels

According to the characteristics of the operator, we divide the shape functions
  in three different modes: data independent, data dependent, and upper bound.
\begin{itemize}
  \item {\em Data independent} shape functions are used for operators in which the output shape
        only depends on the shapes of inputs such as normal 2-D convolution.
  \item {\em Data dependent} shape functions require the concrete input values to compute the output shapes.
        For example, the output shape of \texttt{arange} depends on the value of start, stop, and step.
  \item In addition, there are certain operators such as Non Maximum Suppression (\texttt{nms}) where the
        complexity of computing the output shapes is on par with the complexity of executing the operator itself.
  In order to avoid the redundant computation, we use an {\em upper bound} shape function to quickly estimate an upper bound shape for the output.
  We also require such operators to return the output shape along with output value, so as to use the real shape to slice the output tensors into precise output shape and layout.
\end{itemize}

It is worth noting that in the presence of dynamic shape functions,
  operator fusion needs to be specially taken care of.
However, we only define the shape function as TIR expressions.
As a result, we also must fuse shape functions in
  parallel with the operator fusion.
The compiler can easily connect the shape
  functions of basic operators to form the shape function
  for a composite operator when all shape functions are data independent.
However, a basic operator with a data dependent or upper bound shape
  function cannot be fused to other operators,
  i.e., taking the outputs of other operators as its inputs to fuse together,
  as the shape function requires to access to the intermediate result
  within a composite operator.
As a result, we explicitly define the fusion policy to prevent
  this from happening.
Due to our definition of shape functions we are able to perform fusion
  using a generalized version of the algorithm used for
  standard operator fusion which handles passing the
  appropriate input shape or input depending on the shape function.
For example imagine that the fuse operator has a single input that
  was originally used an operator that requires the input,
  and the second fused operator requires the input shape.
Depending on the type of the shape function defined above,
  we handle operator fusion in the following manner.
Any data independent shape functions could be fused with other operators.
Any fused group can only have at most one shape function and it
  should be fused together with the operator that needs shape function.
