\chapter{Relay: An IR for deep learning}
\label{ch:relay}

Popular DL compiler intermediate representations (IRs) offer different tradeoffs
between expressivity, composability, and portability~\citep{
  tensorflow, pytorch_ad, chainer_learningsys2015, tangent, theano, glow}.
Early frameworks adopted IRs
  specialized for then-state-of-the-art models and/or
  emerging hardware accelerators.
As a result, non-trivial extensions require
  patching or even forking frameworks~\citep{
    tf_fold, tf_lite, tangent, tf_eager, xla, glow, torchscript}.
Such \textit{ad hoc} extensions can improve expressivity
  while maintaining backwards compatibility with existing execution mechanisms.
However, they are difficult to design, reason about, and implement,
  often resulting in modifications that are mutually incompatible.
Nearly all popular deep learning representations were designed for
  static computation graphs, leading to numerous
  extensions designed to support dynamic neural networks.

This thesis present Relay,
  a new compiler IR for deep learning.
Relay's functional, statically typed intermediate representation (IR)
  unifies and generalizes existing DL IRs
  to express state-of-the-art models.
The introduction of Relay's expressive IR requires
  careful design of domain-specific optimizations,
  addressed via Relay's extension mechanisms.
Using these extension mechanisms,
  Relay supports a unified compiler that
  can target a variety of hardware platforms.
Relay's extensible compiler
   can eliminate abstraction overhead and
   target new hardware platforms.

Previous IRs have struggled to address these challenges, treating each
  component of the framework as a disconnected set of programming tasks.
Operators are defined in low-level languages like C++,
  connected by a dataflow graph, and then scripted
  in a host language like Python.
Consequently
  program analyses cannot cross language boundaries between components,
  inhibiting optimization and deployment.
Relay's design takes inspiration from traditional
  compiler literature where many of the challenges facing
  machine learning compilers have been well studied in the scalar setting.
We analyzed previous deep learning IRs finding ways
  to obtain the desirable properties of these IRs
  with a principled approaches, for example using references
  to split pure and in-pure fragments, or the use of closures
  to represent complex control operators.

Relay is also designed to abstract over platform specific behaviors but not prevent
  representing or optimizing for them.
Given a known target, a user can schedule a new optimization,
  and all necessary platform optimizations and code generation will occur.
Target independence might seem like a property already enjoyed widely,
  but in many frameworks each operator is implemented per platform,
  and often models only work on a single well-supported platform (i.e Nvidia GPU).
Previous IRs are either designed to be tethered to a specific end-user programming model
    or low-level operator library which enables the programs to be executed on specific platforms such as GPU.
Leveraging these features leads to powerful use cases,
  for example we are able to easily get best in class performance on many devices by mixing and matching TVM,
  with native kernel libraries to obtain the best performance, without the end user needing to adapt their program
  in anyway.
The rest of this chapter describes Relayâ€™s IR and type system design, and presents some preliminary
  performance results.
We focus on the following contributions:
\begin{itemize}
  \item The Relay IR, a tensor-oriented, statically typed
    functional IR,
    which we describe in this chapter.
  Relay's design is motivated by the insight that functional IRs, used by
  languages from the ML family\footnote{``ML'' as in ``Meta Language,'' not
  ``Machine Learning''} can be readily adapted to support DL.
  Collections of \textit{ad hoc} extensions in previous frameworks
    that patched shortcomings in expressiveness are subsumed by a handful of well-known language
    constructs like let expressions, ADTs, first-class functions, and references.
  In addition to improving expressivity,
    incorporating these features as language constructs
    allows optimizations to more readily compose.
  \item
  By representing DL models as functional programs, we reframe traditional
    deep learning framework features as compiler problems.
  Backpropagation becomes a source code transformation,
    transforming an arbitrary Relay function into its gradient function;
    \textit{ad hoc} shape inference becomes principled type inference;
    graph rewriting becomes program optimization;
    and the executor becomes (depending on what the context demands) an
    interpreter, virtual machine, or ahead-of-time compiler.
  By using this reframing we can tap into
    decades of traditional compilers research to design
    \textit{composable} optimization passes.
  \item
    A platform-agnostic representation of operators and domain specific
      optimizations which work in concert to provide \textit{portability}
      across hardware backends.
\end{itemize}

\section{IR}

\input{chapters/short_grammar.tex}

The Relay IR is a high-level, functional, differentiable language.
Relay is designed to support
  complex models while abstracting over hardware-specific
  implementation details to enable hardware agnostic program
  analysis and optimization.
Rather than invent an entirely new language,
  Relay's IR design is based on IRs used by the well-studied ML family of
  functional programming languages (e.g., SML and OCaml).
These IRs are expressive enough to capture general-purpose programs
  (including control flow, first-class functions, and data types)
  and have clearly specified semantics (e.g., lexical scope and controlled effects).
By borrowing from PL literature,
  we can apply program analysis and optimization techniques from decades of research~\citep{haskell_vector}.
Relay's IR takes a small functional core and enriches it with domain-specific additions---namely,
  the inclusion of tensors and operators as expressions
  and a novel tensor type system design to support tensor shapes.
Our principled design
  enables the import of existing models from deep learning frameworks and exchange formats,
  the implementation of a number of domain-specific optimizations,
  and efficient deployment across a variety of targets.
The Relay IR is designed
  to subsume the functionality of computation graph-based IRs
  while providing greater faculties for abstraction, and dynamism.
We present Relay's design by incrementally building up to the full IR
  starting from a subset that corresponds to a simple computation graph.

Hence, Relay's primary value type is a tensor and operators are included as language primitives
  (see the \verb|tensor constant| and \verb|operator| rules in Figure \ref{fig:short_bnf}).
Relay leaves the implementation of each operator opaque; the operators
  are represented by a lower-level IR, which is optimized independently.
A computation graph, in its simplest presentation, is a directed acyclic
  graph with multiple inputs and a single output.
The syntax of an equivalent computation graph is realized by
  a language with three rules (1) \verb|variable|s, (2) function \verb|call|s,
  and (3) \verb|operator|s, see Figure~\ref{fig:short_bnf} for the corresponding rules.

\subsection{Operators}

Operators are the primitive computation of Relay and represented using
  an opaque function signature, and backed by fragments of TVM's TE or TIR
  which can be lowered to low-level device specific code to realize
  the abstract operations with specialized implementations.

\subsection{Multiple Outputs}

Many common operators like \verb|split|, which splits
  a tensor along a particular axis, require multiple outputs.
In order to handle these programs,
  computation graph IRs have added primitive support
  for multiple outputs.
Multiple outputs can be modeled as tuples, which can
  be added with just two rules (1) \verb|tuple formation|
  and (2) \verb|tuple projection|.
Instead of using multiple outputs as a builtin concept the use
  of tuples allow us to represent operations which not only return
  one level of data structure, but can also use nested structures
  such as tuples of tuples.

\subsection{Let}

By construction, computation graphs enjoy implicit sharing of subcomputations
  via multiple outgoing dependency edges.
Implicit sharing is often implemented via pointers that uniquely identify subgraphs,
  a property useful for both execution and analysis.
Previous frameworks often obtain this sharing by using a host
  language's name binding to construct a graph (e.g., by binding a Python variable
  to a subgraph and using that variable to construct other subgraphs).
General-purpose programming languages, on the other hand, provide \textit{explicit}
  sharing via binding constructs, such as \verb|let|.
In programs free of scope, ordering, and effects, implicit sharing
  and explicit sharing are semantically equivalent.
However, in practice, user programs rely on effects and ordering,
  requiring previous approaches to provide workarounds.
For example, TensorFlow's Eager Mode inserts dummy control edges
  in its generated graphs to impose effect ordering.
The lack of lexical scope in computation graphs complicates language features,
  like first-class functions and control flow,
  and reduces the precision of traditional analyses,
  such as liveness,
  because the high-level program structure is absent~\citep{funarg, funarg_sol}.
The addition of a humble \verb|let| binding, a central concept in functional languages,
  provides explicit sharing and a solution to the problems outlined above.

\subsection{Control Flow}

Emerging models, particularly in the domain of natural language processing,
  increasingly rely on data-dependent control flow,
  forcing frameworks based on computation graph IRs
  to incorporate control flow, often through \textit{ad hoc} and difficult-to-extend constructs.
For example, TensorFlow Fold~\citep{tf_fold} extends TF with special combinators that
  dynamically compute a graph for each shape permutation;
  these high-level constructs are opaque to further optimizations.
Even in the presence of control flow-free models, looping
  constructs are necessary to implement optimization algorithms
  such as SGD.
The central challenge is a flexible and extensible encoding of
  control flow operations.
It is well known in functional programming literature that recursion and pattern matching are sufficient
  to implement arbitrary combinators for control flow and iteration (e.g., maps, folds, and scans).
To support the definition of functional combinators
  we enrich Relay with two more language
  features to implement arbitrary combinators: \verb|if| and first-class recursive functions.

\subsection{First-Class Functions}

A computation graph is a single expression
  from multiple inputs (i.e. its free variables) to multiple outputs.
While it may be tempting to reinterpret a graph as a function, it lacks functional abstraction
  and named recursion.
Adding the ability to name functions and pass them as first-class values dramatically increases
  Relay's expressivity, allowing it to encode generic
  higher-order functions and readily use techniques used in functional
  compilers like automatic deforestation.
First-class functions enable passes such as
  automatic differentiation, and simplify
  the framework importers which map higher-level programs to our IR~\citep{myia}.
For example, an instance of TensorFlow's looping construct \verb|tf.while_loop|
  can be represented as a single specialized loop function
  or a generic fold over the full loop state.
See Figure~\ref{fig:tf_to_relay_loop} for an example of this conversion (via
  the Relay TensorFlow frontend).

\subsection{Data Abstraction}
Many models make use of additional data types beyond
  tuples, such as lists, trees, and graphs~\citep{char-rnn, tree_lstm, graph_lstm}.
Relay borrows from functional languages
  a generic and principled method of extension:
  algebraic data types (ADTs).
To support them, we add mechanisms for
  (1) type declaration and
  (2) pattern matching.
This final addition results in a strict functional language,
  closely resembling the core of languages like OCaml and SML.
The increase in expressivity introduced by the Relay IR introduces
  new optimizations challenges, which we
  discuss in Chapter~\ref{ch:related}.

% --------------------------------------------------------------------------------------------------------
The resulting language is a familiar strict functional language,
  resembling the core of languages like OCaml and SML.
Our language makes domain-specific deviations from existing work,
  and we have provided a full listing
  of its syntax, operational semantics, and type rules
  in the appendix.
A functional language provides a few notable advantages.
Its pure fragment represents idealized computation graphs free
  from effects. This fragment can be easily optimized by end users who
  can reason about it as pure dataflow.
For this reason, Relay is pure by default but exposes a limited
  form of mutation via ML-style references that we have
  primarily used for automatic differentiation.

Relay is more expressive than many previous frameworks and this expressivity introduces new challenges.
  Previous essential functionality such
    as shape inference and automatic differentiation must be adapted for
    our new IR.
How does one reason about the shapes of operators when the input is unknown?
How does one backpropagate over pattern-matching, control, data types, and mutation?
In the following subsection we demonstrate how one can adapt techniques
  from type inference and checking to Relay.

% Another important semantic choice is that all Relay operators expose a functional interface, i.e they consume inputs and outputs.
% The functional semantics allow us to later introduce mutable buffers without requiring complex interprocedural program analysis.
% Our second principle was a transparent separation from the abstract representation of machine learning models, and kernels.
% The abstraction is successfully introduced by computation graph IRs, but in an opaque way which limits cross layer optimization.
% Many gestalt approaches use a single language that defines all kernels and models, requiring users to both reason about all levels during optimization.

% We are able to represent abstract operations with well known types at the Relay level via the introduction of a user extensible shape dependent
%   type system which can represent a wide range of operatorâ€™s types.

% Our final guiding principle has been target independence.
\input{chapters/represent/tf_figure.tex}

\input{chapters/represent/type_system.tex}
\input{chapters/represent/eval.tex}
