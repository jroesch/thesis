\chapter{Relay: An IR for deep learning}
\label{ch:relay}

Popular DL compiler intermediate representations (IRs) offer different tradeoffs
between expressivity, composability, and portability~\citep{
  tensorflow, pytorch_ad, chainer_learningsys2015, tangent, theano, glow}.
Extending these frameworks to accommodate
  the rapidly diversifying landscape of
  DL models and hardware platforms presents
  challenging tradeoffs between
  expressivity, composability, and portability.
Early frameworks adopted IRs
  specialized for then-state-of-the-art models and/or
  emerging hardware accelerators.
As a result, non-trivial extensions require
  patching or even forking frameworks~\citep{
    tf_fold, tf_lite, tangent, tf_eager, xla, glow, torchscript}.
Such \textit{ad hoc} extensions can improve expressivity
  while maintaining backwards compatibility with existing execution mechanisms.
However, they are difficult to design, reason about, and implement,
  often resulting in modifications that are mutually incompatible.
Nearly all popular deep learning representations were designed for
  static computation graphs, leading to numerous of these types of
  extensions to support dynamic program features.

We present Relay,
  a new compiler framework for DL.
Relay's functional, statically typed intermediate representation (IR)
  unifies and generalizes existing DL IRs
  to express state-of-the-art models.
The introduction of Relay's expressive IR requires
  careful design of domain-specific optimizations,
  addressed via Relay's extension mechanisms.
Using these extension mechanisms,
  Relay supports a unified compiler that
  can target a variety of hardware platforms.
Relay's extensible compiler
   can eliminate abstraction overhead and
   target new hardware platforms.

Previous IRs have struggled to address these challenges, treating each
  component of the framework as a disconnected set of programming tasks.
Operators are defined in low-level languages like C++,
  connected by a dataflow graph, and then scripted
  in a host language like Python.
Consequently,
  program analyses cannot cross language boundaries between components,
  inhibiting optimization and deployment.
Learning from previous IRs, we have designed Relay,
  which features a principled approach to addressing extensibility
  and improves expressivity, composability, and portability
  over previous frameworks.

  Previous IRs have struggled to address these challenges, treating each
    component of the framework as a disconnected set of programming tasks.
  Operators are defined in low-level languages like C++,
    connected by a dataflow graph, and then scripted
    in a host language like Python.
  Consequently,
    program analyses cannot cross language boundaries between components,
    inhibiting optimization and deployment.
  Learning from previous IRs, we have designed Relay,
    which features a principled approach to addressing extensibility
    and improves expressivity, composability, and portability
    over previous frameworks.


Previous IRs have struggled to address these challenges because they treat
    each part of the framework as a disconnected set of libraries.
  Operators are defined in low-level languages like C++,
    connected with a dataflow graph, and then scripted
    in a host language such as Python;
    analyses cannot traverse the boundaries between these components.
  Learning from the steps and missteps of previous IRs, we designed Relay,
    a principled approach to addressing extensibility.
  By treating the entire stack as a language
    design problem, Relay enables reasoning across
    abstraction boundaries, which
    improves expressivity, composability, and portability over previous frameworks.
% The principle of least semantic dictates we should make design choices which give further optimization stages
%   the ability to perform optimizations without unnecessary detail.
% For example enabling implicit effects and non-explicit control complicates computational graph semantics,
%   and limits optimization.
% TF’s DAG attempts to separate pure semantics in order to handle out of order execution, and parallel and distributed scheduling,
%   but does so by compromising on the programming model.

Instead we were able to build an IR inspired by the ML family of languages which provides pure-sub fragments which can be scheduled
  the same way without compromising on the programming model.

Another important semantic choice is that all Relay operators expose a functional interface, i.e they consume inputs and outputs.
The functional semantics allow us to later introduce mutable buffers without requiring complex interprocedural program analysis.
Our second principle was a transparent separation from the abstract representation of machine learning models, and kernels.
The abstraction is successfully introduced by computation graph IRs, but in an opaque way which limits cross layer optimization.
Many gestalt approaches use a single language that defines all kernels and models, requiring users to both reason about all levels during optimization.

We are able to represent abstract operations with well known types at the Relay level via the introduction of a user extensible shape dependent
  type system which can represent a wide range of operator’s types.

Our final guiding principle has been target independence.

Relay was designed as a platform independent representation of machine learning computation.
Given a known target, a user can schedule a new optimization,
  and all necessary platform optimizations and code generation will occur.
Target independence might seem like a property already enjoyed widely,
  but in many frameworks each operator is implemented per platform,
  and often models only work on a single well-supported platform (i.e Nvidia GPU).
Previous IRs are either designed to be tethered to a specific end-user programming model
    or low-level operator library which enables the programs to be executed on specific platforms such as GPU.

Leveraging these features leads to powerful use cases, for example we are able to easily get best in class performance on many devices
by mixing and matching TVM, with native kernel libraries to obtain the best performance, without the end user needing to adapt their program
in anyway, see section N for more details.

There are many more examples we will cover in the next few sections of how Relay’s semantics enable novel solutions for domain-specific optimization.
The rest of the section examines Relay’s design and properties which set it apart from previous solutions.

We make the following contributions:
\begin{itemize}
  \item The Relay IR, a tensor-oriented, statically typed
    functional IR,
    which we describe in this chapter.
  Relay's design is motivated by the insight that functional IRs, used by
  languages from the ML family\footnote{``ML'' as in ``Meta Language,'' not
  ``Machine Learning''} can be readily adapted to support DL.
  With its \textit{expressive} semantics,
    including control flow, data structures, and first-class functions,
    Relay can represent entire state-of-the-art models.
  \item The insight that common features in ML frameworks,
    such as quantization and shape inference,
    can be reframed as standard compiler passes.
  By using this reframing we can tap into
    decades of traditional compilers research to design
    \textit{composable} optimization passes.
  \item
    A platform-agnostic representation of operators and domain specific
      optimizations which work in concert to provide \textit{portability}
      across hardware backends.
\end{itemize}

We make the following core contributions:
\begin{itemize}
  \item
  Below we describe Relay, a tensor-oriented, statically typed,
    functional IR.
  Collections of \textit{ad hoc} extensions in previous frameworks
    that patched shortcomings in expressiveness are subsumed by a handful of well-known language
    constructs like let expressions, ADTs, first-class functions, and references.
  In addition to improving expressivity,
    incorporating these features as language constructs
    allows optimizations to more readily compose.
  \item
  By representing DL models as functional programs, we reframe traditional
    DL framework problems as compiler problems.
  Backpropagation becomes a source code transformation,
    transforming an arbitrary Relay function into its gradient function;
    \textit{ad hoc} shape inference becomes principled type inference;
    graph rewriting becomes program optimization;
    and the executor becomes (depending on what the context demands) an
    interpreter, virtual machine, or ahead-of-time compiler.
  Using this correspondence, we adapt existing
    PL techniques to the DL domain.
  \item
    A notable example of this approach is Relay's type system (Section \ref{sec:type_system}).
    Since operators have complicated semantics, shape inference is usually
      performed when shapes are fully concrete;
      however, at compile time, one does not have that luxury.
    We therefore extend a Hindley-Milner type system with type relations that encode shape
      constraints induced by operators.
    This allows Relay passes to reason about shape information at compile time.
  \item To provide portability,
    we define a platform-agnostic operator language
    and a compiler pass manager, which facilitates the development of
    passes that transform the IR to target new hardware backends.
\end{itemize}

Relay is an open source component of the \tvm project.
It has been deployed at Amazon Web Services, Facebook, Huawei,
  and is currently being adopted by teams at ARM and Qualcomm.
It is used to deploy efficient machine learning to
  both commodity and custom hardware with minimal
  engineering effort.

\section{IR}

Relay's expressive high-level IR is designed to support
  complex models while abstracting over hardware-specific
  implementation details to enable hardware agnostic program
  analysis and optimization.
Rather than invent an entirely new language,
  Relay's IR design is based on IRs used by the well-studied ML family of
  functional programming languages (e.g., SML and OCaml).
These IRs are expressive enough to capture general-purpose programs
  (including control flow, first-class functions, and data types)
  and have clearly specified semantics (e.g., lexical scope and controlled effects).
By borrowing from PL literature,
  we can apply program analysis and optimization techniques from decades of research~\citep{haskell_vector}.

Relay's IR takes a small functional core and enriches it with domain-specific additions---namely,
  the inclusion of tensors and operators as expressions
  and a novel tensor type system design to support tensor shapes.
Our principled design
  enables the import of existing models from deep learning frameworks and exchange formats,
  the implementation of a number of domain-specific optimizations,
  and efficient deployment across a variety of targets.
In the remainder of this section,
  we describe the IR design in further detail
  and explore the ramifications of this design on the compilation stack.

The Relay IR is designed
  to subsume the functionality of computation graph-based IRs
  while providing greater faculties for abstraction and control flow.
We present Relay's design by incrementally building up to the full IR
  starting from a subset that corresponds to a simple computation graph.
Deep learning models fundamentally operate on tensors.
Hence, Relay's primary value type is a tensor and operators are included as language primitives
  (see the \verb|tensor constant| and \verb|operator| rules in Figure \ref{fig:short_bnf}).
Relay leaves the implementation of each operator opaque; the operators
  are represented by a lower-level IR, which is optimized independently.
A computation graph, in its simplest form, is a directed acyclic
  graph with multiple inputs and a single output.
Relay uses three constructs to support these simple graphs:
  (1) \verb|variable|, (2) function \verb|call|,
  and (3) \verb|operator|; see Figure~\ref{fig:short_bnf} for the corresponding rules.

\subsection*{Multiple Outputs}

Computation graph IRs have primitive support for multiple outputs
  because many tensor operators require it.
For example, the \verb|split| operator separates a tensor along a given axis
  and returns each component.
In Relay, multiple outputs can be modeled as tuples,
  requiring only two rules: \verb|tuple formation| and \verb|tuple projection|.

\subsection*{Let}

By construction, computation graphs enjoy implicit sharing of subcomputations
  via multiple outgoing dependency edges.
Implicit sharing is often implemented via pointers that uniquely identify subgraphs,
  a property useful for both execution and analysis.
Previous frameworks often obtain this sharing by using a host
  language's name binding to construct a graph (e.g., by binding a Python variable
  to a subgraph and using that variable to construct other subgraphs).
General-purpose programming languages, on the other hand, provide \textit{explicit}
  sharing via binding constructs, such as \verb|let|.
In programs free of scope, ordering, and effects, implicit sharing
  and explicit sharing are semantically equivalent.
However, in practice, user programs rely on effects and ordering,
  requiring previous approaches to provide workarounds.
For example, TensorFlow's Eager Mode inserts dummy control edges
  in its generated graphs to impose effect ordering.
The lack of lexical scope in computation graphs complicates language features,
  like first-class functions and control flow,
  and reduces the precision of traditional analyses,
  such as liveness,
  because the high-level program structure is absent~\citep{funarg, funarg_sol}.
The addition of a humble \verb|let| binding, a central concept in functional languages,
  provides explicit sharing and a solution to the problems outlined above.

\subsection*{Control Flow}


Emerging models, particularly in the domain of natural language processing, increasingly
  rely on data-dependent control flow, forcing frameworks based on computation graph IRs
  to incorporate control flow, often through \textit{ad hoc} and difficult-to-extend constructs.
For example, TensorFlow Fold~\citep{tf_fold} extends TF with special combinators that
  dynamically compute a graph for each shape permutation;
  these high-level constructs are opaque to further optimizations.
The functional programming community has demonstrated that recursion and pattern matching are sufficient
  to implement arbitrary combinators for control flow and iteration (e.g., maps, folds, and scans).
To support the definition of functional combinators
  we enrich Relay with two more language
  features to implement arbitrary combinators: \verb|if| and first-class recursive functions.

\subsection*{First-Class Functions}

A computation graph is a single computation
  from multiple inputs to multiple outputs.
While it is tempting to reinterpret a graph as a function,
  graphs lack functional abstraction and named recursion.
The addition of first-class named functions dramatically increases
  Relay's expressivity, allowing it to encode generic
  higher-order functions and thus capture higher-level program structure.
First-class functions also enable simpler implementations
  of importers that map higher-level programs to our IR.
For example, an instance of TensorFlow's looping construct \verb|tf.while_loop|
  can be represented as a single specialized loop function
  or a generic fold over the loop state.
See Figure~\ref{fig:tf_to_relay_loop} for an example of this conversion (via
  the Relay TensorFlow frontend).

\subsection*{Data Abstraction}
Many models make use of additional data types beyond
  tuples, such as lists, trees, and graphs~\citep{char-rnn, tree_lstm, graph_lstm}.
Relay borrows from functional languages
  a generic and principled method of extension:
  algebraic data types (ADTs).
To support them, we add mechanisms for
  (1) type declaration and
  (2) pattern matching.
This final addition results in a strict functional language,
  closely resembling the core of languages like OCaml and SML.
The increase in expressivity introduced by the Relay IR introduces
  new optimizations challenges, which we
  discuss in Chapter~\ref{ch:related}.


  \subsection{IR}
  \input{chapters/short_grammar.tex}

  The Relay IR is a high-level, functional, differentiable language.
  One can understand Relay by starting from a subset of Relay
    that represents an idealized computation graph IR and
    incrementally growing to the full Relay IR.
  A computation graph, in its simplest presentation, is a directed acyclic
    graph with multiple inputs and a single output.
  The syntax of an equivalent computation graph is realized by
    a language with three rules (1) \verb|variable|s, (2) function \verb|call|s,
    and (3) \verb|operator|s, see Figure~\ref{fig:short_bnf} for the corresponding rules.

  \subsection{Multi-Output}

  This subset lacks useful features that are present in IRs used
    in practice.
  For example, common operators such as \verb|split|, which splits
    a tensor along a particular axis, require multiple outputs.
  In order to handle these programs,
    computation graph IRs have added primitive support
    for multiple outputs.
  Multiple outputs can be modeled as tuples, which can
    be added with just two rules (1) \verb|tuple formation|
    and (2) \verb|tuple projection|.

  \subsection{Let}

  By construction, computation graphs enjoy implicit sharing of subcomputations via multiple outgoing
  dependency edges.
  Implicit sharing is useful for both execution and analysis,
    because it enables users to uniquely identify subgraphs.
  Previous frameworks often obtain sharing by using a host
    language's name binding to construct a graph.
  General purpose programming languages, on the other hand, provide \textit{explicit}
    sharing via binding constructs, such as \verb|let|.
  In programs free of scope, ordering, and effects, implicit sharing
    and explicit sharing are semantically equivalent.
  However, in the presence of these three, implicit sharing does not adequately preserve the semantics
    of effects, since their ordering is not well-defined.
  Since user programs contain scope, ordering, and effects in practice,
    previous systems have been forced to provide workarounds.

  For example, TensorFlow's eager mode inserts dummy control edges
    in its generated graphs to impose an ordering on effects.
  The lack of lexical scope in traditional graphs complicates language features
    such as first-class functions and control-flow~\citep{funarg, funarg_sol}.
  The lack of explicit scoping information also weakens the ability
    to provide precise versions of traditional analyses, such as liveness.
  The addition of a humble \verb|let| binding, well studied in programming languages,
    enables explicit sharing and provides an elegant solution to the problems outlined above.

  \subsection{Control Flow}

  \begin{figure}[htb!]
    \begin{tabular}{ccc}
    \begin{minipage}{0.5\textwidth}
    \begin{minted}[fontsize=\small]{python}
  i = tf.constant(1)
  j = tf.constant(1)
  k = tf.constant(5)

  def c(i, j, k):
    return
      tf.equal(
        tf.not_equal(
          tf.less(i + j, 10),
          tf.less(j * k, 100)),
         tf.greater_equal(k, i + j))

  def b(i, j, k): return [i+j, j+k, k+1]

  tf.while_loop(c, b, loop_vars=[i, j, k])
    \end{minted}
    \end{minipage}
  & \hspace{-3.0em}
  \begin{Huge}
    $\Rightarrow$
  \end{Huge}
  &
    \begin{minipage}{0.5\textwidth}
    \begin{minted}[fontsize=\footnotesize]{python}
  let %while_loop =
    fn (%loop_var0: Tensor[(1,), int32],
        %loop_var1: Tensor[(1,), int32],
        %loop_var2: Tensor[(1,), int32]) {
      %0 = add(%loop_var0, %loop_var1)
      %1 = less(%0, meta[Constant][0])
      %2 = multiply(%loop_var1, %loop_var2)
      %3 = less(%2, meta[Constant][1])
      %4 = not_equal(%1, %3)
      %5 = add(%loop_var0, %loop_var1)
      %6 = greater_equal(%loop_var2, %5)
      if (min(equal(%4, %6))) {
        %9 = add(%loop_var0, %loop_var1)
        %10 = add(%loop_var1, %loop_var2)
        %11 = add(%loop_var2, meta[Constant][2])
        %while_loop(%9, %10, %11)
      } else {
        (%loop_var0, %loop_var1, %loop_var2)
      }
    }
  %while_loop(meta[Constant][3],
              meta[Constant][4],
              meta[Constant][5])
    \end{minted}
    \end{minipage}
    \end{tabular}
    \caption{
      A simple TensorFlow loop in the user-facing DSL and the Relay
        loop produced by automatically converting it.
      Note the TensorFlow while loop corresponds neatly to a tail recursive
        function.
      The Relay text format supports a ``metadata'' section which functions
        as a constant pool among other things.
      \texttt{meta[Constant][n]} represents the \texttt{n}-th constant in the
        pool.
    }
    \label{fig:tf_to_relay_loop}
    \end{figure}


% \begin{figure*}[htb!]
%   \begin{tabular}{ccc}
%   \begin{minipage}{0.4\textwidth}
%   \begin{minted}[fontsize=\small]{python}
% i = tf.constant(1)
% j = tf.constant(1)
% k = tf.constant(5)

% def c(i, j, k):
%   return
%     tf.equal(
%       tf.not_equal(
%         tf.less(i + j, 10),
%         tf.less(j * k, 100)),
%         tf.greater_equal(k, i + j))
% def b(i, j, k): return [i+j, j+k, k+1]
% tf.while_loop(c, b, loop_vars=[i, j, k])
%   \end{minted}
%   \end{minipage}
% & \hspace{-2.0em}
% \begin{Huge}
%   $\Rightarrow$
% \end{Huge}
% &
%   \begin{minipage}{0.5\textwidth}
%   \begin{minted}[fontsize=\footnotesize]{python}
%   fn %while_loop(
%     %lvar0: Tensor[(1,), int32], %lvar1: Tensor[(1,), int32],
%     %lvar2: Tensor[(1,), int32]) {
%     %0 = add(%lvar0, %lvar1)
%     %1 = less(%0, meta[Constant][0])
%     %2 = multiply(%lvar1, %lvar2)
%     %3 = less(%2, meta[Constant][1])
%     %4 = not_equal(%1, %3)
%     %5 = add(%lvar0, %lvar1)
%     %6 = greater_equal(%lvar2, %5)
%     if (min(equal(%4, %6))) {
%       %9 = add(%lvar0, %lvar1)
%       %10 = add(%lvar1, %lvar2)
%       %11 = add(%lvar2, meta[Constant][2])
%       %while_loop(%9, %10, %11)
%     } else { (%lvar0, %lvar1, %lvar2)
%     }
%   }
%   %while_loop(meta[Constant][3], meta[Constant][4], meta[Constant][5])
%   \end{minted}
%   \end{minipage}
%   \end{tabular}
%   \caption{\textmd{
%     A simple TensorFlow loop in the user-facing DSL and the Relay
%       loop produced by automatically converting it.
%     Note the TensorFlow while loop corresponds neatly to a tail recursive
%       function.
%     The Relay text format supports a ``metadata'' section which functions
%       as a constant pool among other things.
%     \texttt{meta[Constant][n]} represents the \texttt{n}-th constant in the
%       pool.
%   }}
%   \label{fig:tf_to_relay_loop}
%   \end{figure*}

  Neural networks increasingly rely on control flow, forcing frameworks based on computation graph IRs
  to support this construct; however, control flow extensions are generally \textit{ad hoc}.
  Even in the presence of control flow-free models, looping
    constructs are necessary to implement optimization algorithms
    such as SGD.
  Furthermore emerging architectures are beginning to make greater use of
     control flow, with many architectures exposing custom control
     combinators such as loops, maps, folds, and scans.
  The central challenge is a flexible and extensible encoding of
    control flow operations.
  The functional programming community has demonstrated recursion and pattern matching are sufficient
    to implement arbitrary combinators for control flow and iteration.
  To support the definition of functional loops we enrich Relay with two more language
    features to implement arbitrary combinators: \verb|if| and first-class recursive functions.
  The guard expression in Relay's \verb|if| expression operates over rank-0 boolean tensors,
    which represent booleans.

  \subsection{First-Class Functions}

  A computation graph is a single expression
    from multiple inputs (i.e. its free variables) to multiple outputs.
  While it may be tempting to reinterpret a graph as a function, it lacks functional abstraction
    and named recursion.
  Adding the ability to name functions and pass them as first-class values dramatically increases
    Relay's expressivity, allowing it to encode generic
    higher-order functions and readily use techniques used in functional
    compilers like automatic deforestation.
  First-class functions enable passes such as
    automatic differentiation, and simplify
    the framework importers which map higher-level programs to our IR~\citep{myia}.
  For example, an instance of TensorFlow's looping construct \verb|tf.while_loop|
    can be represented as a single specialized loop function
    or a generic fold over the full loop state.
  See Figure~\ref{fig:tf_to_relay_loop} for an example of this conversion (via
    the Relay TensorFlow frontend).

  \subsection{Data Abstraction}
  Earlier, we extended the language with tuples to
    emulate behavior of existing IRs.
  Deep networks require additional data types like lists,
    trees, and graphs~\citep{char-rnn, tree_lstm, graph_lstm}.
  Relay borrows a generic and principled way to
    extend a language with new data types:
    algebraic data types (ADTs).
  To support them we add (1) a type declaration mechanism and
    (2) pattern matching.
  One may question why Relay has \verb|if| when it could be subsumed by \verb|match|:
    \verb|if| is still necessary because tensors are primitives, not ADTs.
    % and thus do not have a recursion principle that characterizes their match behavior.

  The resulting language is a familiar strict functional language,
    resembling the core of languages like OCaml and SML.
  Our language makes domain-specific deviations from existing work,
    and we have provided a full listing
    of its syntax, operational semantics, and type rules
    in the appendix.
  A functional language provides a few notable advantages.
  Its pure fragment represents idealized computation graphs free
    from effects. This fragment can be easily optimized by end users who
    can reason about it as pure dataflow.
  For this reason, Relay is pure by default but exposes a limited
    form of mutation via ML-style references that we have
    primarily used for automatic differentiation.

  Relay is more expressive than many previous frameworks and this expressivity introduces new challenges.
    Previous essential functionality such
     as shape inference and automatic differentiation must be adapted for
     our new IR.
  How does one reason about the shapes of operators when the input is unknown?
  How does one backpropagate over pattern-matching, control, data types, and mutation?
  In the following subsection we demonstrate how one can adapt techniques
    from type inference and checking to Relay.

\input{chapters/represent/type_system.tex}
\input{chapters/represent/eval.tex}s
