\section{Evaluation}
\label{sec:eval}

We evaluated Relay on several systems (x86 CPUs, ARM CPUs, NVIDIA GPUs, Xilinx FPGAs) and over
  diverse vision and NLP workloads to demonstrate that (1) Relay enables \emph{composability} of
  graph-level optimizations, (2) Relay delivers \emph{performance} on inference tasks competitive
  with state-of-the-art frameworks (TensorFlow, PyTorch, MxNet), and (3) Relay provides
  \emph{portability} over difficult-to-compile-to hardware backends such as FPGAs

We evaluated the following vision models:
  \textit{Deep Q-Network (DQN)}, a DNN that achieved state-of-the-art performance
  on 49 Atari games in 2015;
  \textit{MobileNet}, a DNN designed for image recognition on mobile and
  embedded devices;
  \textit{ResNet-18}, a DNN for image recognition that achieved state-of-the-art
  performance on ImageNet detection tasks in 2015;
  \textit{VGG-16} (named for the Visual Geometry Group
  at Oxford), a CNN used for image recognition tasks
  \citep{dqn, mobilenet, resnet, vgg}.

We evaluated the following NLP models:
  \textit{CharRNN}, a generator character-level
  RNN from a PyTorch tutorial;
  \textit{TreeLSTM}, a generalization of LSTMs to
  tree-structured network topologies;
  \textit{RNN, GRU, and LSTM}, a selection of models from the Gluon
  model zoo
  \citep{pytorch_rnn_tut, tree_lstm, gluon_model_zoo}.

\subsection{Experimental Methodology}
  Because we only evaluate inference in this paper,
    we frequently make use of random inputs to models when measuring
    performance.
  There were two exceptions where we evaluated on real data because
    it was readily available: CharRNN and TreeLSTM.

  For models where the input is random,
    we run 1000 timed iterations.
  Before the timed runs,
    we run 8 untimed ``warm-up'' iterations to ensure any caching and JIT compilation
    employed in lower levels of the stack are included in the 1000 timed runs.
  This way,st
    the timed runs reflect the \textit{stable} performance of the system.
  For our purposes, ``performance'' refers to end-to-end framework time on
    inference tasks (i.e., the time it takes to run a trained model) in a
    single-machine setting.

  Our vision experiments from Chapter~\ref{ch:related} and Section~\ref{sec:perf-gpu} were run on a machine with an AMD Ryzen
    Threadripper 1950X 16-Core CPU,
    an NVidia 1080 Ti GPU,
    and 64 GB of RAM.
  Our NLP experiments from Section~\ref{sec:perf-gpu} were run on a machine with an AMD Ryzen
    Threadripper 1950X 16-Core CPU,
    an NVidia Titan-V GPU,
    and 64 GB of RAM.
  We evaluated Relay's handling of accelerators on a VTA design with a
    $16\times16$ matrix-vector 8-bit tensor core clocked at 333MHz on the Ultra-96 platform.

  In terms of software, we used
    Cuda version 10.0,
    CuDNN version 7.5.0,
    TVM commit \texttt{cefe07e2a}\footnote{NLP experiments required custom modifications that may be made public later},
    MxNet version 1.4.0,
    Pytorch version 1.0.1post2,
    and TensorFlow version 1.13.1.

  The Relay vision experiments utilized aggressively tuned TVM schedules on the GTX 1080 Ti GPU,
    improving performance significantly.

  \subsection{Vision Workloads}
  \label{sec:perf-gpu}

  \begin{figure}[h]
    \includegraphics[width=
    \textwidth]{fig_splash19/eval/vision_1080Ti_relay.pdf}
    \caption{
      Inference slowdown of popular frameworks relative to Relay on vision
        benchmarks running on NVIDIA GTX 1080 Ti GPUs.
      Relay provides performance competitive to the state of the art.
      We ran 1000 trials for each model and used the AoT compiler.
    }
    \label{fig:vision-eval}
  \end{figure}

  \begin{figure}[h]
    \includegraphics[width=
    \textwidth]{fig_splash19/eval/nlp_TitanV_relay.pdf}
    \caption{
      Inference slowdown relative to Relay on NLP benchmarks running on NVIDIA
        Titan-V GPUs.
      NLP workloads feature control flow,
        which makes them more challenging to optimize.
      Relay provides performance competitive to state of the art (up to
        2.4$\times$ speedup over MxNet on GRU).
      We ran 1000 trials for each model, except for CharRNN, on which we used 100 trials.
    }
    \label{fig:nlp-eval}
  \end{figure}

  An age-old story in compilers literature is that increasing expressivity
    impacts the global performance of the system.
  We set out to build zero-cost abstractions for Relay,
    governed by Stroustrup's principle, ``What you don't use, you don't pay
    for'' \citep{bjarne}.
  We demonstrate that we can achieve competitive performance on both CPUs and
    GPUs on a wide set of CNNs that are well supported by existing frameworks.
  We evaluated inference time for two classes of workloads: computer vision and natural language processing.
  We compared Relay (using our AoT compiler) to \nnvm,
    TensorFlow, TensorFlow-XLA (Accelerated Linear Algebra), PyTorch, and MxNet.
  We ran the vision and NLP workloads on GTX 1080 Ti and Titan-V GPUs, respectively.

  \subsection{Vision Evaluation}
  % Figure~\ref{fig:vision-eval} compares Relay against state of the art frameworks
  %   running vision workloads on a GTX 1080 Ti GPU.
  We ran each model with
    batch size 1, a common setting in inference tasks.
  Relay achieves performance on par with \nnvm,
    an existing deep learning graph compiler in use at Amazon.
  Relay outperforms TensorFlow, TensorFlow-XLA, MxNet and
    PyTorch on every benchmark.
  Relay's ability to do aggressive optimizations like operator
    fusion on long chains of operations, generating hardware
    specific implementations, enables it to outperform
    existing frameworks that don't perform inter-operator optimizations.

  \subsection{NLP Evaluation}
  % Figure~\ref{fig:vision-eval} compares Relay against state-of-the-art NLP models on a Titan-V GPU.
  % Implementations of the NLP models were not available in all frameworks;
    we used MxNet baselines for RNN, GRU, and LSTM and PyTorch for Char-RNN and TreeLSTM.
  % We ran the models for 1000 iterations per input, except char-RNN, which we ran for 100 ???.
  % To run the RNN, GRU, and LSTM benchmarks in MxNet, and Char-RNN, and TreeLSTM
  %   in PyTorch.
  Relay performs better than MxNet on recursive models
    due to the fact they are implemented in Python using
    MxNet's looping constructs.
  PyTorch instead uses handwritten and heavily optimized
    C implementations of the recursive network cells.
  Due to this we perform slightly \emph{worse} than PyTorch.
  It is interesting to note that our pure Relay
    implementation performs competitively against
    the hand-optimized version.

  \subsection{Targeting Deep Learning Accelerators on FPGAs}

  \begin{figure}[h]
    \includegraphics[width=\textwidth]{fig_splash19/eval/vision_fpga.pdf}
    \caption{
      Inference time (ms) of vision DNNs on Ultra-96 FPGA-enabled SoC.
      We compare vision workloads that Relay compiles onto the embedded Cortex
        A53 CPU vs. a DNN accelerator implemented on the integrated FPGA fabric.
      Targeting DNN accelerators can unlock up to 11x speedups, but requires a
        multitude of graph-level transformations.
      We used 10 trials for each model.
    }
    \label{fig:fpga-eval}
  \end{figure}

  We evaluated inference time on five models including MobileNet-G \citep{mobilenet}, a grouped variant of the MobileNet architecture; ResNet-18, ResNet-34, and ResNet-50\citep{resnet}; and Deep Convolutional Generative Adversarial Networks \citep{dcgan}, a generative DNN used in unsupervised learning.
  Overall, Relay helps us efficiently offload deep learning operators onto specialized accelerators like VTA.
  Our results in Figure~\ref{fig:fpga-eval} show that we can achieve between 2.5 to 11.7$\times$ reduction in single-batch inference latency by offloading critical operators to the FPGA accelerator.
  These experiments demonstrate Relay's ability to target current and future deep learning architectures:
  \begin{enumerate}
    \item \textit{Heterogeneous FPGA/CPU offloading}: Relay lets us define the rules for offloading specific operators to the FPGA-based accelerator.
    \item \textit{Push-button quantization}: Relay can take a \texttt{fp32} model and convert its parameters to \texttt{int8} in order to enable inference on specialized accelerators.
    \item \textit{Accelerator-friendly data packing:} Relay reorganizes data so it can be effortlessly consumed by a specialized TPU-like accelerator~\citep{tpuv1}.
  \end{enumerate}
