\section{Evaluation}
\label{sec:eval}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% WISDOM FROM THE PARODY REVIEWER %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Explain in the caption of the quantization figure that the table shows that we have a
% quant scheme that doesn't degrade accuracy by very much and the graph shows that
% it gives significant performance gains.
%
% Explain in the caption of the FPGA figure that we can then use quantization and
% other IR transformations to target FPGAs *period*.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure*}[htp!]
  \centering
  \includegraphics[scale=0.53]{fig/eval/graph/pass_comp/pass-comp-cpu.png}
  \includegraphics[scale=0.53]{fig/eval/graph/pass_comp/pass-comp-gpu.png}
  \caption{\textmd{
    Speedup from successively layering compiler passes in \relay on CPU
    (AMD Ryzen Threadripper 1950X) and GPU (Nvidia Titan-V),
      relative to no optimizations at all.
    The ``Op Fusion'' bars represent the application of operator fusion,
      the ``... + Constant Folding'' bars represent the application of operator fusion \textit{and} constant folding,
      and so on.
    The full list of passes used is as follows:
      \textit{operator fusion};
      \textit{constant folding};
      \textit{operator layout alteration}, which transforms the data layouts of operators for better cache performance;
      and \textit{common subexpression elimination}.
    We find that composing passes can steadily increase performance.
    The effectiveness of each pass is both model- and device-dependent.
    In particular,
      the most effective passes for CPU and GPU are operator layout alteration and operator fusion,
      respectively.
  }}
  \label{fig:composability-eval}
\end{figure*}

We evaluate \relay's ability to provide expressivity, composability, and portability,
  without compromising on performance.
In particular, our evaluation is composed of three parts:
\begin{enumerate}
  \item \textbf{\relay expresses diverse workloads}: Despite increasing
    expressiveness, \relay's performance is competitive with the
    state of the art on popular models.
  \item \textbf{\relay enables composable optimizations}: \relay
    supports composing program transformations to incrementally improve performance.
  \item \textbf{\relay handles challenging backends}: \relay can compile
    models to execute efficiently on a variety of
    backends, such as FPGA accelerators, which require quantization, layout
    optimizations, and bit-packing transformations.
\end{enumerate}

We evaluated the following vision models:
  \textit{Deep Q-Network (DQN)}, a CNN that achieved state-of-the-art performance
  on 49 Atari games in 2015;
  \textit{MobileNet}, a CNN designed for image recognition on mobile and
  embedded devices;
  \textit{ResNet-18}, a CNN for image recognition that achieved state-of-the-art
  performance on ImageNet detection tasks in 2015;
  and \textit{VGG-16} (named for the Visual Geometry Group at Oxford),
    a CNN for image recognition that achieved top-2 performance in the 2014 ImageNet Challenge.
  % and \textit{Deep Convolutional Generative Adversarial Networks (DCGAN)},
  %   a generative CNN used in unsupervised learning.
  \citep{dqn, mobilenet, resnet, vgg}.
We evaluated the following NLP models:
  \textit{CharRNN}, a generator character-level
  RNN from a PyTorch tutorial;
  \textit{TreeLSTM}, a generalization of LSTMs to
  tree-structured network topologies;
  and \textit{RNN}, \textit{GRU}, and \textit{LSTM}, a selection of models from the Gluon
  Model Zoo
  \citep{pytorch_rnn_tut, tree_lstm, gluon_model_zoo}.

\subsection{Experimental Methodology}
Because we only evaluate inference in this paper,
  we frequently make use of random inputs to models when measuring
  performance.
There were two exceptions where we evaluated on real data because
  it was readily available: CharRNN and TreeLSTM.
For each experiment,
  we run 10 untimed ``warm-up'' iterations to ensure any effects from caching and
  JIT compilation are excluded from the timed runs.
The vision and NLP experiments (from Section~\ref{sec:eval_opts} and Section~\ref{sec:perf-gpu})
  were run on a machine with an AMD Ryzen Threadripper 1950X 16-Core CPU,
  an Nvidia Titan-V GPU,
  and 64 GB of RAM.
For the vision workloads,
  we used TVM's graph runtime as the executor,
  and for the NLP workloads,
  we used \relay's AoT compiler.
The low-power vision experiments from Section~\ref{sec:low-power} were run on multiple edge-class ARM development boards: a Raspberry Pi 3, a Firefly RK3399, and an Pynq-Z1 FPGA platform.
We implement our DNN accelerators on a Zynq-7020 low-cost FPGA, and clock them at 100MHz.
% We evaluated \relay's handling of accelerators on VTA~\citep{moreau2018vta}, the versatile
%   open-source deep learning accelerator.
% We implemented a VTA design with a $16\times16$ matrix-vector 8-bit tensor core clocked
%   at 333MHz on the Ultra-96 platform.
We used the following software versions:
  CUDA version 10.0,
  CuDNN version 7.5.0,
  TVM commit \texttt{e518fe1c}\footnote{NLP experiments required extensions to the MxNet importer that will be made public later},
  MxNet version 1.5.0,
  PyTorch version 1.2.0,
  and TensorFlow version 1.14.0.

\subsection{\relay Expresses Diverse Workloads}
\label{sec:perf-gpu}

An age-old story in compilers literature is that increasing expressivity
  impacts the global performance of the system.
We set out to build zero-cost abstractions for \relay,
  governed by Stroustrup's principle, ``What you don't use, you don't pay
  for'' \citep{bjarne}.
We demonstrate that we achieve competitive performance on a wide set of CNNs that are well supported by existing frameworks.
We evaluated inference time for two classes of workloads: computer vision and natural language processing.
We compared \relay to \nnvm,
  TensorFlow, TensorFlow-XLA (Accelerated Linear Algebra), PyTorch, and MxNet.
Results are summarized in Figure~\ref{fig:expressivity-eval}.

\subsection*{Vision Evaluation}

\begin{figure*}[htbp!]
  \centering
  \includegraphics[scale=0.68]{fig/eval/graph/vision_comp/cnn-comp-gpu.png}
  \includegraphics[scale=0.68]{fig/eval/graph/nlp_comp/nlp-comp-cpu.png}
  \includegraphics[scale=0.68]{fig/eval/graph/vision_comp/legend.png}
  \caption{\textmd{
    Inference speedup of \relay relative to popular frameworks
      on vision and NLP benchmarks.
    The vision benchmarks used an NVIDIA Titan-V GPU, and the NLP benchmarks ran on CPU only.
    We ran 1000 trials for each model, except for CharRNN, on which we used 100 trials.
    \relay matches the performance of NNVM on vision but additionally supports NLP,
      where \relay provides performance competitive to the state of the art (up to
        2.3$\times$ speedup over MxNet on GRU).
  }}
  \label{fig:expressivity-eval}
\end{figure*}

We ran each model with
  batch size 1, a common setting in inference tasks.
\relay achieves performance on par with \nnvm
  and outperforms TensorFlow, TensorFlow-XLA, MxNet, and
  PyTorch on every benchmark.
\relay's ability to apply aggressive inter-operator optimizations
  enables it to outperform existing frameworks.
Operator fusion over long chains of operations is particularly effective,
  because it can generate \textit{new} hardware-specific fused implementations.

\subsection*{NLP Evaluation}
% TODO: We need to explain the difference between "RNN" (the Gluon one) and
% "CharRNN". I don't think *we* even know what the diff is.
Implementations of the NLP models were not available in all frameworks;
  we used MxNet baselines for RNN, GRU, and LSTM and PyTorch for CharRNN and TreeLSTM.
NLP workloads feature control flow,
  which makes them more challenging to optimize.
\relay performs better than MxNet on GRU and LSTM
  because they are implemented in Python using
  MxNet's looping constructs.
However,
  MxNet outperforms \relay on the Gluon RNN,
  because it uses a hardcoded optimization to unroll the RNN,
  whereas \relay expresses it as a loop without any unrolling.
PyTorch instead uses handwritten and heavily optimized
  C implementations of the recursive network cells.
Despite this,
  our pure \relay implementation outperforms PyTorch by 1.4$\times$ on
  CharRNN and 2$\times$ on TreeLSTM.
This speedup comes from \relay's ability to compile \textit{entire}
  models with complex control flow (e.g., CharRNN) to a single lean binary.

\subsection{\relay Enables Composable Optimizations}
\label{sec:eval_opts}

We demonstrate that \relay facilitates composable optimizations
  by evaluating vision workloads under both general-purpose and DL-specific compiler passes.
Figure~\ref{fig:composability-eval} shows mean inference speedup relative to
  no optimizations as \relay applies optimizations more aggressively.
We find that performance gains vary significantly between each device-model pairing.
Most networks benefit greatly from operator layout alteration on CPU and operator fusion on GPU.
On both CPU and GPU,
  VGG-16 is resistant to most optimizations,
  because the architecture primarily consists of back-to-back convolutions,
  which are not fusable.
Elementwise operations \textit{can} be fused,
  so we see greater improvements on ResNet and MobileNet,
  which both feature elementwise adds from residual connections.
It's unsurprising that MobileNet fares so well on CPU---it is \textit{designed} to run well on CPU.
Nature-DQN has simple operators,
  which don't benefit from layout alterations,
  whereas ResNet-18 and VGG-16 are dense convolutional neural networks,
  which \textit{do} benefit from layout transformations.
Overall, these results show that \relay lets us compose optimizations
  in a way that is beneficial to diverse workloads.

\subsection{\relay Handles Challenging Backends}
\label{sec:low-power}
To demonstrate portability,
  we evaluate two sets of optimizations:
    those that are merely \textit{beneficial} for low-power platforms and
    those that are \textit{necessary} to target hardware accelerators.

\subsection*{Quantized Inference on ARM Platforms}
To demonstrate the effectiveness of our generic quantization (see Section~\ref{sec:quant}),
  we use \relay to evaluate both \textit{accuracy} and \textit{performance} of different
  quantization schemes on vision workloads.
To evaluate \textit{accuracy},
  we tested various quantization schemes
  (denoted $m/n$ for $m$-bit quantization and $n$-bit accumulation)
  against a \texttt{float32} baseline on three vision models,
  as shown in the table below:
\begin{center}
  \begin{tabular}{|c|c||c|c||c|c|}
    \hline
    \multicolumn{2}{|c}{\textbf{ResNet-18}} & \multicolumn{2}{c}{\textbf{MobileNet V2}} & \multicolumn{2}{c|}{\textbf{Inception V3}} \\
    \multicolumn{1}{|c}{QS}    & \multicolumn{1}{c}{Acc.}   &  \multicolumn{1}{c}{QS}  & \multicolumn{1}{c}{Acc.}  & \multicolumn{1}{c}{QS}  & \multicolumn{1}{c|}{Acc.} \\
    \hline
    \texttt{fp32} & 70.7 \%    & \texttt{fp32} & 70.9 \%       & \texttt{fp32} & 76.6 \% \\
    8/32         & 69.4 \%    & 8/32         & 66.9 \%       & 16/32        & 76.6 \% \\
    8/32         & 69.4 \%    & 8/16         & 66.9 \%       & 8/32         & 75.2 \% \\
    \hline
  \end{tabular}
\end{center}
Figure~\ref{fig:portability-eval} shows the results of different
  levels of quantization on \textit{performance} when applied to the Raspberry Pi 3
  and Firefly RK3399 ARM-based platforms.
The numbers show that as we opt for a more aggressive quantization scheme
  (e.g., 8/16),
  we achieve much improved performance with hardly a drop in accuracy.
Interestingly,
  on some model/platform pairs,
  the \texttt{int8/int32} scheme performs slightly worse than \texttt{float32} on both platforms,
  which likely stems from the existence of faster hardware intrinsics for 16-bit operations on these systems.

\subsection*{Targeting Deep Learning Accelerators on FPGAs}
% Suppose a company wishes to accelerate inference by offloading DL operators onto a specialized accelerator.
% The first design they try supports \textit{single-batch} inference.
% As they become familiar with the characteristics of their workloads,
%   they realize they can increase throughput by batching inputs,
%   so they move to a design that supports \textit{multi-batch} inference.
% In any popular framework,
%   supporting even \textit{one} new accelerator is treading off the beaten path,
%   let alone iterating on \textit{multiple} designs.
We demonstrate that \relay can support specialized hardware by compiling vision and
NLP workloads onto two DNN accelerator designs (\texttt{single-batch}, \texttt{multi-batch}) we generate in-house.
%
% The first DNN accelerator implements \texttt{single-batch} inference by exposing a \emph{vector-matrix}
% multiplication intrinsic.
%., where the vector and matrix shapes are $(1, 16)$, and $(16, 16)$ respectively.
%
% The second DNN accelerator implements \texttt{multi-batch} inference by exposing a \emph{matrix-matrix}
% multiplication intrinsic.
%, where the vector and matrix shapes are $(4, 8)$, and $(8, 8)$ respectively.
%
The two DNN designs have the same number of MAC (multiply and accumulate) units which are
arranged differently to expose different compute intrinsics to the compiler.
%
% This forces the software compiler to re-arrange the compute graph and data layout
% to match the hardware intrinsics constraints.
%
% For instance the \texttt{multi-batch} and \texttt{single-batch} designs impose a $NCHWnc$
% and $NCHWc$ layout constraint respectively on the convolution activations of CNNs.

We evaluate batch-size-normalized inference time on the accelerator designs
  on a mix of vision and NLP workloads:
  ResNet-18, ResNet-34, ResNet-50, ResNet-101 \citep{resnet};
  and TreeLSTM in Figure~\ref{fig:portability-eval}.
The ResNets have high arithmetic intensity
due to 2D convolutions, while the NLP workload is memory bound due to the lower-intensity vector-matrix
multiplication used in the LSTM cells.

We show that running the workloads
  on the \texttt{multi-batch} DNN accelerator improves throughput on all workloads at the cost
  of naturally increasing inference latency.
%
Batching is not compelling on ResNet because it increases the arithmetic
  intensity of a workload that is already compute-bound.
%
On the other hand, TreeLSTM presents a compelling target for batching due to the
  memory bound nature of the LSTM cell computation.
  % : by applying batching, we increase the arithmetic intensity
  % of the workload which leads to pending less time waiting for data on the accelerator.
%
While these two accelerator variants have the same peak throughput on paper, we show that \relay let us
  evaluate more nuanced end-to-end performance numbers across different workloads.
% We demonstrate that \relay is a strict improvement over other frameworks in this scenario,
%   because the data layout constraints imposed by accelerator hardware can be expressed in
%   \relay to take full advantage of hardware specialization.

% TODO: Fill in analysis once we have results.
% Our results in  latency and ??? to
%   ???$\times$ reduction in multi-batch inference latency by offloading critical
%   operators to the FPGA accelerator.

These experiments demonstrate \relay's ability to target current and future deep learning architectures,
and make informed decision on what hardware design to choose across different workloads.


% \begin{enumerate}
%   \item \textit{Heterogeneous FPGA/CPU offloading}: \relay lets us define the rules for offloading specific operators to the FPGA-based accelerator.
%   \item \textit{Push-button quantization}: \relay can take a \texttt{fp32} model and convert its parameters to \texttt{int8} in order to enable inference on specialized accelerators.
%   \item \textit{Accelerator-friendly data packing:} \relay reorganizes data so it can be effortlessly consumed by a specialized TPU-like accelerator~\citep{tpuv1}.
% \end{enumerate}

% We compared \relay (using \tvm's graph runtime environment) to \nnvm and TensorFlow
% on a Raspberry Pi 3 ARM CPU and a Titan X GPU.
% Additionally, we also compared against the TensorFlow (v1.7) XLA (Accelerated Linear Algebra) compiler \citep{xla}, backed by cuDNN v7
% on the GPU and TensorFlow Lite (commit: 7558b085) \citep{tf_lite}, TensorFlow's mobile-optimized framework, backed by the ARM Compute Library (v18.03)
% on the ARM CPU.
% We ran each model 100 times on each target with batch size 1, a common
% setting in inference tasks.
% The results of the above comparison are given in Fig.~\ref{fig:cnn-eval}.
% \relay achieves performance on par with \nnvm on DQN and MobileNet, though it is approximately
% 1.7 times slower than \nnvm on ResNet-18. In spite of this \relay outperforms TensorFlow
% on every benchmark. We investigated this slowdown and were able to verify that this
% difference is due to a single layout optimization \nnvm performs that is not yet implemented
% in \relay.
% \subsection{\relay Improves Performance for Dynamic Models}
% We demonstrate how \relay can be used to compile the style of programs written by PyTorch
% programmers, and achieve strong performance results at the same time.
% We compared \relay to PyTorch on a set of generative Char-RNNs \citep{char-rnn}, in order
% to use them as a generate model we required control flow, recursion, and ADTs.
% We evaluate these results on a computer with 16-core AMD Ryzen Threadripper 1950X Processor.
% When using a Char-RNN to perform text generation we use a recurrent cell executed in a loop.
% We evaluated this model in four configurations:
% \begin{enumerate}[label=(\arabic*)]
%   \item \relay interprets the Char-RNN cell and the outer loop runs in Python (similar to default PyTorch);
%   \item \relay AoT compiles the Char-RNN cell and the outer loop runs in Python (similar to PyTorch's JIT compiler, \citep{glow});
%   \item \relay interprets the entire model; and
%   \item \relay ahead-of-time compiles the entire model.
% \end{enumerate}
% Char-RNN additionally has a hidden layer whose size can be adjusted before the network is run.
% We also evaluate performance on varying sizes of the hidden layer.
% The results of the above-described experiment are given in Fig.~\ref{fig:rnn-eval}. Three of
% the \relay configurations are 2-4 times slower then PyTorch, but with good reason: These three
% configurations rely on the reference \relay interpreter, which can not compare with Python's
% ByteCode VM. By applying \relay's ahead-of-time compiler to the program we are able to achieve
% nearly a factor of 3$\times$ improvement on PyTorch's results for all hidden layer sizes.
% \subsection{\relay Enables Composable Passes}
% \todo{
% copy from Steven's report, but the passes we want to compose are quantization
% with other passes, to show we don't suffer from the same problems as TF Lite.
% We want to compose AD, PE, an DCE to get performance.
% }
% \subsection{\relay Handles Customized Hardware}
% Finally we evaluated \relay's ability to target custom accelerators.
% Custom accelerators like Google's Cloud TPU~\citep{tpuv1} present a challenge to computational graph compilers for two reasons.
% First, accelerators favor narrow integer types for inference tasks, requiring graph compiler support for quantization from \texttt{fp32} down to \texttt{int8} types or lower.
% Second, data has to be \emph{packed} and \emph{tiled} in order to match the data layout requirements imposed by the computational core of the accelerator.
% It is common to find tensor computation lanes in accelerators that can perform single-cycle matrix-matrix or matrix-vector multiplication.
% To sustain this tremendously high rate of computation, the data has to be arranged correctly in memory to match the specifications of the accelerator's tensor core.
% The data layout constraints imposed by accelerator hardware can be expressed in \relay to take full advantage of hardware specialization.
% We evaluated \relay's handling of accelerators on VTA~\citep{moreau2018vta}, the versatile open-source deep learning accelerator.
% We implemented a VTA design with a $16\times16$ matrix-vector 8-bit tensor core clocked at 333MHz on the Ultra-96 development board.
% The Ultra-96 is based on a Zynq Ultrascale+ MPSoC, which comprises an ARM Cortex A53 CPU and an 16nm FinFET+ Xilinx FPGA.
% These MPSoCs offer tight integration between the CPU and FPGA via a shared, coherent memory interface.
% This lets us perform fine-grained offloading of operators with \relay.
% Our ability to target FPGA-based accelerators demonstrates three points:
% \begin{enumerate}
%   \item \textit{Heterogeneous FPGA/CPU offloading}: \relay lets us define when it makes sense to offload certain operators to the FPGA. Not all operators can efficiently be mapped to the specialized accelerator, and their evaluation should be able to fall back onto the CPU.
%   \item \textit{Push-button quantization}: \relay can take a \texttt{fp32} model and convert its parameters to \texttt{int8} in order to facilitate inference on specialized accelerators.
%   \item \textit{Accelerator-friendly data packing:} \relay reorganizes data in memory so it can be effortlessly consumed by a specialized accelerator.
% \end{enumerate}
% We evaluated inference time on five models including MobileNet-G \citep{mobilenet}, a grouped variant of the MobileNet architecture, ResNet-18, ResNet-34 and ResNet-50\citep{resnet}, and Deep Convolutional Generative Adversarial Networks \citep{dcgan}, a generative DNN used in unsupervised learning.
% Overall, \relay helps us efficiently offload deep learning operators onto specialized accelerators.
% Our results in Figure~\ref{fig:fpga-eval} show that we can achieve between 2.4 to 11.7$\times$ reduction in single-batch inference latency by offloading critical operators to the FPGA accelerator.
% These experiments demonstrate \relay's ability to target future architectures.
% \begin{figure*}{h}
% \begin{tabu} to \textwidth { | X[l] | X[c] | X[c] | X[c] | X[c] | X[c] | X[c] | X[c] | X[c] | }
%     \hline
%     Framework & Control Flow & Recursion & Abstraction & Static Typing & Higher Order Diff. & Full AOT & No-External Op Library & Accelerator Support \\
%     \hline
%     TensorFlow & \checkmark & \Cross & \Cross & \Cross & \Cross & \checkmark & \Cross & \checkmark \\
%     PyTorch & \checkmark & \checkmark & \checkmark & \Cross & \Cross & \checkmark & \Cross & \Cross \\
%     MxNet & \checkmark & \checkmark & \checkmark & \Cross & \Cross & \checkmark & \Cross & \Cross \\
%     Flux.jl + Zygote.jl & \checkmark & \checkmark & \checkmark & \Cross & \Cross & \checkmark & \Cross & \Cross \\
%     \relay{} & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark \\
%    \hline
% \end{tabu}
% \caption{The above table compares the functionality and support of various deep learning compilers being built in industry,
%          and academia. We compare the frameworks along criteria that match well with the properties laid out in the paper's
%          introduction.}
% \end{figure*}
