\section{Design}
\label{sec:design}

\relay's expressive high-level IR is designed to support
  complex models while abstracting over hardware-specific
  implementation details to enable hardware agnostic program
  analysis and optimization.
Rather than invent an entirely new language,
  \relay's IR design is based on IRs used by the well-studied ML family of
  functional programming languages (e.g., SML and OCaml).
These IRs are expressive enough to capture general-purpose programs
  (including control flow, first-class functions, and data types)
  and have clearly specified semantics (e.g., lexical scope and controlled effects).
By borrowing from PL literature,
  we can apply program analysis and optimization techniques from decades of research~\citep{haskell_vector}.

\relay's IR takes a small functional core and enriches it with domain-specific additions---namely,
  the inclusion of tensors and operators as expressions
  and a novel tensor type system design to support tensor shapes.
Our principled design
  enables the import of existing models from deep learning frameworks and exchange formats,
  the implementation of a number of domain-specific optimizations,
  and efficient deployment across a variety of targets.
In the remainder of this section,
  we describe the IR design in further detail
  and explore the ramifications of this design on the compilation stack.

\subsection{IR}
\input{chapters/short_grammar.tex}

The \relay IR is designed
  to subsume the functionality of computation graph-based IRs
  while providing greater faculties for abstraction and control flow.
We present \relay's design by incrementally building up to the full IR
  starting from a subset that corresponds to a simple computation graph.
Deep learning models fundamentally operate on tensors.
Hence, \relay's primary value type is a tensor and operators are included as language primitives
  (see the \verb|tensor constant| and \verb|operator| rules in Figure \ref{fig:short_bnf}).
\relay leaves the implementation of each operator opaque; the operators
  are represented by a lower-level IR, which is optimized independently.
A computation graph, in its simplest form, is a directed acyclic
  graph with multiple inputs and a single output.
\relay uses three constructs to support these simple graphs:
  (1) \verb|variable|, (2) function \verb|call|,
  and (3) \verb|operator|; see Figure~\ref{fig:short_bnf} for the corresponding rules.

\subsection*{Multiple Outputs}

Computation graph IRs have primitive support for multiple outputs
  because many tensor operators require it.
For example, the \verb|split| operator separates a tensor along a given axis
  and returns each component.
In \relay, multiple outputs can be modeled as tuples,
  requiring only two rules: \verb|tuple formation| and \verb|tuple projection|.

\subsection*{Let}

By construction, computation graphs enjoy implicit sharing of subcomputations
  via multiple outgoing dependency edges.
Implicit sharing is often implemented via pointers that uniquely identify subgraphs,
  a property useful for both execution and analysis.
Previous frameworks often obtain this sharing by using a host
  language's name binding to construct a graph (e.g., by binding a Python variable
  to a subgraph and using that variable to construct other subgraphs).
General-purpose programming languages, on the other hand, provide \textit{explicit}
  sharing via binding constructs, such as \verb|let|.
In programs free of scope, ordering, and effects, implicit sharing
  and explicit sharing are semantically equivalent.
However, in practice, user programs rely on effects and ordering,
  requiring previous approaches to provide workarounds.
For example, TensorFlow's Eager Mode inserts dummy control edges
  in its generated graphs to impose effect ordering.
The lack of lexical scope in computation graphs complicates language features,
  like first-class functions and control flow,
  and reduces the precision of traditional analyses,
  such as liveness,
  because the high-level program structure is absent~\citep{funarg, funarg_sol}.
The addition of a humble \verb|let| binding, a central concept in functional languages,
  provides explicit sharing and a solution to the problems outlined above.

\subsection*{Control Flow}

\begin{figure*}[htb!]
  \begin{tabular}{ccc}
  \begin{minipage}{0.4\textwidth}
  \begin{minted}[fontsize=\small]{python}
i = tf.constant(1)
j = tf.constant(1)
k = tf.constant(5)

def c(i, j, k):
  return
    tf.equal(
      tf.not_equal(
        tf.less(i + j, 10),
        tf.less(j * k, 100)),
       tf.greater_equal(k, i + j))
def b(i, j, k): return [i+j, j+k, k+1]
tf.while_loop(c, b, loop_vars=[i, j, k])
  \end{minted}
  \end{minipage}
& \hspace{-2.0em}
\begin{Huge}
  $\Rightarrow$
\end{Huge}
&
  \begin{minipage}{0.5\textwidth}
  \begin{minted}[fontsize=\footnotesize]{python}
  fn %while_loop(
    %lvar0: Tensor[(1,), int32], %lvar1: Tensor[(1,), int32],
    %lvar2: Tensor[(1,), int32]) {
    %0 = add(%lvar0, %lvar1)
    %1 = less(%0, meta[Constant][0])
    %2 = multiply(%lvar1, %lvar2)
    %3 = less(%2, meta[Constant][1])
    %4 = not_equal(%1, %3)
    %5 = add(%lvar0, %lvar1)
    %6 = greater_equal(%lvar2, %5)
    if (min(equal(%4, %6))) {
      %9 = add(%lvar0, %lvar1)
      %10 = add(%lvar1, %lvar2)
      %11 = add(%lvar2, meta[Constant][2])
      %while_loop(%9, %10, %11)
    } else { (%lvar0, %lvar1, %lvar2)
    }
  }
  %while_loop(meta[Constant][3], meta[Constant][4], meta[Constant][5])
  \end{minted}
  \end{minipage}
  \end{tabular}
  \caption{\textmd{
    A simple TensorFlow loop in the user-facing DSL and the \relay
      loop produced by automatically converting it.
    Note the TensorFlow while loop corresponds neatly to a tail recursive
      function.
    The \relay text format supports a ``metadata'' section which functions
      as a constant pool among other things.
    \texttt{meta[Constant][n]} represents the \texttt{n}-th constant in the
      pool.
  }}
  \label{fig:tf_to_relay_loop}
  \end{figure*}

Emerging models, particularly in the domain of natural language processing, increasingly
  rely on data-dependent control flow, forcing frameworks based on computation graph IRs
  to incorporate control flow, often through \textit{ad hoc} and difficult-to-extend constructs.
For example, TensorFlow Fold~\citep{tf_fold} extends TF with special combinators that
  dynamically compute a graph for each shape permutation;
  these high-level constructs are opaque to further optimizations.
The functional programming community has demonstrated that recursion and pattern matching are sufficient
  to implement arbitrary combinators for control flow and iteration (e.g., maps, folds, and scans).
To support the definition of functional combinators
  we enrich \relay with two more language
  features to implement arbitrary combinators: \verb|if| and first-class recursive functions.

\subsection*{First-Class Functions}

A computation graph is a single computation
  from multiple inputs to multiple outputs.
While it is tempting to reinterpret a graph as a function,
  graphs lack functional abstraction and named recursion.
The addition of first-class named functions dramatically increases
  \relay's expressivity, allowing it to encode generic
  higher-order functions and thus capture higher-level program structure.
First-class functions also enable simpler implementations
  of importers that map higher-level programs to our IR.
For example, an instance of TensorFlow's looping construct \verb|tf.while_loop|
  can be represented as a single specialized loop function
  or a generic fold over the loop state.
See Figure~\ref{fig:tf_to_relay_loop} for an example of this conversion (via
  the \relay TensorFlow frontend).

\subsection*{Data Abstraction}
Many models make use of additional data types beyond
  tuples, such as lists, trees, and graphs~\citep{char-rnn, tree_lstm, graph_lstm}.
\relay borrows from functional languages
  a generic and principled method of extension:
  algebraic data types (ADTs).
To support them, we add mechanisms for
  (1) type declaration and
  (2) pattern matching.
This final addition results in a strict functional language,
  closely resembling the core of languages like OCaml and SML.
The increase in expressivity introduced by the \relay IR introduces
  new optimizations challenges, which we
  discuss in Sec.~\ref{sec:optimizations}.

\subsection{Type System}
\label{subsec:type_system}

\relay's type system is essential
  to optimizations.
Typing guarantees both well-formedness of the program
  and provides crucial tensor shape information to perform allocation,
  check correctness, and facilitate loop optimizations.
Shape information is also valuable for data layout transformations and tensorization,
  two transformations often demanded by hardware accelerators.
In computation graph IRs, only numeric data types
  and shapes are tracked for each operator.
Symbolic shapes (i.e., shape polymorphism) are only handled
  dynamically, inhibiting certain types of optimizations.

It is possible to model arbitrarily complex static properties, such
  as shape information, with a dependent type theory~\citep{selsam_certigrad}, but such
  a design incurs significant user complexity.
By incorporating shape analysis into a broader type system,
  \relay's type system balances the desire for static tensor shapes
  with usability.
In this subsection, we describe how to extend a polymorphic type system with shape
  information and type inference with shape inference.

\subsection*{Tensor Types}

The primitive value in \relay is a tensor, which has
  a shape and a base type (\verb|tensor type| in Figure \ref{fig:short_bnf}).
Base types describe the elements of tensors by tracking
  the bit width,
  the number of lanes (for utilizing vectorized intrinsics),
  and whether the type is floating point or integral.
To ensure \relay can offload tensor computation to devices
  with greatly varying architectures,
  \relay tensors may only contain base types,
  preventing, for example, tensors of closures.
The shape of a tensor is a tuple of integers describing the tensor's dimensions.
A dimension may be a variable or arithmetic expression that indicates how the
  output shape of an operator depends on those of its inputs.
Functions may be polymorphic over shapes, which results
  in shape constraints that must be solved during type inference.
Sec.~\ref{sec:inference} describes the process.
\relay also supports a special shape called \verb|Any|, which is used
  to mark a dynamic shape when static relationships are not profitable
  to model.

\subsection*{Operators and Type Relations}
Operators are one of the key primitives that differs from those of
  general-purpose programming languages.
\relay's use of opaque operators enables backends to choose different
  lowering strategies based on the hardware target.
\relay's operator set is extensible, meaning that users may add new operations.
Supporting common or user-defined tensor operators requires a type system that can
  adapt to complex shape relationships between input and output types
  (e.g., elementwise operators with broadcasting semantics).

To handle the constraints between operators' argument shapes, \relay's type system
  introduces type relations.
A type relation is implemented as a function in the
  meta-language and represents a symbolic relationship between
  the input and output types.
When developers add a new operator to \relay, they may constrain its
  type with an existing relation or add their own.
Function types may include
  one or more type relations over a subset of the argument types and the return type.
The type checker enforces that these relationships hold at each call site.

\subsection*{Type Inference}
\label{sec:inference}

To incorporate type relations into \relay's type system, we enrich
  a Hindley-Milner-style type inference algorithm with
  a constraint solver.
\relay's inference algorithm has three steps: first, it
  performs a pass over the AST, generating types and a set of relations,
  then it solves the incurred constraints,
  and finally annotates each sub-expression with its inferred type.

% a queue of relations. dequeue a relation and attempt to solve it by invoking it's opaque
% type-relation function.
% 1. all type variables concrete & relationship holds => solved. discard relation
% 2. all type variables concrete & relationship doesn't hold => type error
% 3. types are partially symbolic => propagate unif. constraints to its inputs and output. if
% all type variables are solved/assigned/concrete, discard relation. otherwise add it and the
% relations that depend on the solved variables to queue. those that are already on the queue should
% be reprioritized

When the type inference algorithm visits a function call site, the function's type relations are
  instantiated with the concrete argument types at the call site.
Each instantiated relation is added to the queue of relations to solve.
The relationship between a call's type variables and relations is added as an edge to
  a bipartite dependency graph where the two disjoint sets are type variables and type relations.
Traditional unification constraints are represented using a modified union-find structure that
  integrates with this dependency graph.

Once the queue is populated, the algorithm will dequeue a relation and attempt to solve it.
There are two cases when solving a type relation:
\begin{enumerate}
  \item If all the relation's type variables
  are concrete, we the relation function. If that function returns true, the
  constraint is discharged. Otherwise, type checking fails.
  \item If any type is fully or partially symbolic, the
    algorithm will propagate
    existing concrete type information via unification.
  All relations affected by new assignments to type
    variables (as determined by the dependency graph)
    are moved to the beginning of the queue.
  If the current type relation is now completely solved, we
  discard it to avoid unnecessarily visiting it again.
\end{enumerate}

We run this to fixpoint or until the queue is empty.
If the queue is non-empty and no progress is made between iterations,
  then at least one variable is underconstrained and inference fails.
Note that a type relation's implementation can
  compromise type soundness, as they are axiomatic descriptions
  of operations implemented outside of \relay.
In practice, the number of type relations needed to express \relay's
  operators is small, and their implementations are straightforward
  and amenable to exhaustive testing.

\subsection{Compiler Framework}

The process for compiling \relay proceeds in three stages.
First, the frontend converts input formats into the \relay IR.
Next, the \relay compiler typechecks and optimizes the program
  to produce the final program.
After performing optimizations,
  the \relay backend transforms
  the \relay program into a form that can be executed on
  the intended hardware, based on the specified execution mechanism.
The backend additionally lowers \relay operators into a TVM expression,
  computes a schedule for the final TVM expression, and lowers it into
  native code.

\subsection*{Frontend}

There are several ways to write an \relay program.
A user can build an in-memory representation of
  a program in C++ or Python,
  parse one written in the \relay text format,
  load one from the on-disk serialization format,
  or import one from popular frameworks and interchange formats
    (e.g., TensorFlow, MxNet, Keras, DarkNet, and ONNX).
Many frameworks and interchange formats use static computation graph-based representations,
  which can easily be translated into \relay.
A greater challenge is translating frameworks
  with a richer computation model such as TensorFlow (TF).
TF supports control flow and includes \verb|TensorArray|, a write-once
  tensor container.
We can extract the loop structure out of the TF graph, converting
  it to an \relay loop, and transform the \verb|TensorArray| into an \relay list.
Once new deep learning languages and IRs under development
  are stable it is likely they can be translated into \relay (see
  Section~\ref{sec:pl_techniques_in_dl}).
PyTorch provides an expressive programming model, and is a good fit
  for \relay, which has integration into PyTorch's JIT infrastructure,
  enabling users to transparently use \relay for improved performance.

\subsection*{Compiler}
Once an \relay abstract syntax tree (AST) is produced,
  the program is optimized by applying a series of \relay-to-\relay
  passes.
Between each pass, \relay performs type inference and checking,
  rejecting malformed programs as well as populating shape and type
  information that passes can utilize.
The \relay compiler supports traditional optimizations
  (e.g., constant folding, common subexpression elimination, and dead code elimination)
  and domain-specific optimizations
  (see Sec.~\ref{sec:optimizations}).

\subsection*{Backends}

\relay produces machine-specific code
  by decomposing the problem of code generation into multiple distinct phases.
\relay translates all operators into \tvm expressions
  to produce dense linear algebra kernels~\citep{tvm_osdi18, tensor_comprehensions, halide}.
\tvm produces low-level operators that expect a fixed calling convention,
  as well as preallocated inputs and outputs.
The result is an object file containing hardware-specific implementations of all
  operations.
The remaining \relay program then is executed or compiled,
  with operator invocations replaced by calls to the optimized operators.
By representing operators as \tvm expressions, we can programmatically
  transform them and automatically generate new implementations for the transformed operators.
Optimizations like fusion and quantization
  rely on this novel behavior.
After primitive operators are lowered,
  the remaining \relay program ties
  together operator invocations, allocation, control-flow,
  recursion, and high-level data structures.
There are multiple options for executing the combined full program:
  the \relay interpreter (with JIT compilation),
  an \relay virtual machine,
  the \tvm graph runtime,
  and an experimental \relay ahead-of-time compiler
  that converts programs to C++ to produce a target-specific binary.
% We are able to target CPU, GPU,
%   iOS and Android mobile devices,
%   custom accelerators, and FPGAs.
