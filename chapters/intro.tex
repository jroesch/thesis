\chapter{Introduction}
\label{ch:intro}

Traditionally machine learning algorithms were carefully engineered by humans
in order to leverage limited data to obtain sufficient task performance.
During the Big Data era, these hand engineered algorithms were displaced by
simpler algorithms applied to the abundantly available data [1].
In order to scale these algorithms a number of systems were
designed most notably MapReduce, Spark and other stream and graph processing systems.
In the early 2010s the emergence of deep learning changed the paradigm again
replacing simple regular computation over streams of data with large models
composed of compute-hungry tensor computations.

This new generation of machine learning applications demanded a corresponding era of
innovation in systems.
The first innovation in deep learning systems were so called
‘deep learning frameworks’ such as TensorFlow and PyTorch.
These monolithic systems can execute a subset of deep learning models
efficiently on a subset of the available hardware devices.
These rigid systems provide good baseline performance, but often fail
when straying from well-used models.

Scaling frameworks to span the scope of all models and targets is a human endeavor
requiring deep expertise often not found in conjunction with the skills of a
machine learning engineer.

A single machine learning engineer cannot write code that scales across the growing
sea of models, operators, optimizations, and devices while maintaining state-of-the-art
performance. From the early days of deep learning frameworks researchers realized the
potential for apply compilation to this problem.

Deep learning compilers as they are often referred to has made similar tradeoffs to
those of frameworks accelerating a subset of models on a subset of devices.

Designing a deep learning compiler even for this constrained subset is challenging.
For example, a popular state-of-the-art DL compiler, Google's XLA, can famously
slow down a programs on CPU instead of speeding them up (cite needed).

Compilers for deep learning are an emerging area explored by a variety of solutions both
industrial and academic.
Part of my thesis work in graduate school was meaningful contributions to
the design of and implementation of deep learning compilers in the form of TVM,
a state-of-the-art deep learning compiler.

In particular I have spent the last several years focused on the representation,
optimization, differentiation, and execution of dynamic neural networks.
In this thesis I propose that we can generalize the overspecialized optimizations
and insights applied to static dataflow graphs to dynamic neural networks by using
one simple insight: dynamic neural networks are not different then traditional
programs. The challenge is how to build a representation that captures this generality
in a principled way allowing us to recover the potential performance loss by generalizing
the programming model.

When I began working on deep learning compilers, input models were high-level
declarative programs composed of linear algebra and statistics primitives.
These straight forward representations often appeared in the form of
a directed acyclic dataflow graph.

We realize a unified interface in the form of Relay a set of systems and APIs designed
  to compose together techniques across the stack to achieve state of the art performance.

Instead of building a single IR and compiler to rule them all we carefully split
  the program optimization problem into a series of phases each focused on a specific
  optimization task.
By doing this we are able to balance the tension between expressivity, composition
  and performance portability.
Relay is a part of the TVM open source project, and catalystized a major redesign of
  TVM, in order to embrace and extend the ideas of this thesis.
Relay is deployed in production at multiple large companies including
  OctoML, Amazon, Facebook, and Microsoft.
One notable impact is its use in
  optimizing Alexa’s wake word model, executed each time a user interacts with
  Alexa.

\section{State of the Art}

Existing state of the art in deep learning compilers falls into one of two categories: a) layered “lego” blocks and “gestalt” solutions, with compilers such as NNVM/TVM, Glow/CuDNN, XLA/CuDNN being examples of the first, and tools such as Lift or Lantern being examples of the second.

Layered systems are designed in a bottom up fashion, starting from the high-performance, device specific kernel libraries provided by hardware manufacturers. The use of an inflexible, and static interface at the bottom-most level has forced the design of higher level components. Practically this has introduced a strict separation between so-called ‘graph-compilers’ and so called ‘kernel libraries’.

These graph compilers implement a simple DAG of operations which they then map down to kernels. The graph compilers are a passable solution, assuming the user can describe their program in a restrictive and awkward language, and only makes use of existing kernels. If the user model makes use of the well-optimized subset of kernels and well-optimized devices it can assume passable performance, but with many models this is not that case, with very little recourse for program authors.

In the kernel domain there has been a large amount of existing work on ensuring individual kernels, or pipelines of them are efficient, but these DLSs are not integrated into a larger programming abstraction. There existing systems like TensorRT which solve this problem for a specific device, with a series of caveats.

Gestalt approaches have attempted to resolve the host of issues with the above design, but have also failed to provide a solution. The best example of this is Lift IR, a functional language designed to bring a principled programming languages approach to generating high-performance code on a variety of devices.

Although Lift provides impressive results for a system built on purely rewriting it provides a single programming model for end users, and requires all optimizations to either be written as a pass or phrased as a set of user rewrites. This design does not provide a way for users to incorporate outside implementations, prior knowledge or compose with their ML pipelines often written in Python.

Existing approaches using both strategies have limitations. Compiler design using the first strategy are limited by each layer only having an opaque view of the layers above and below, each layer uses a single semantics with well-defined mappings between layers but limited communication between, or in the case of a single monolithic IR, which chooses a functional semantics the inability to use semantics which provide hardware specificity or hardware aware semantics.

Imagine a not-so-hypothetical end-user’s desire to support a new Transformer based model on a previously unsupported device such as a microcontroller. In order to obtain best in class performance this task will be split across an engineering team. For example one user with the correct expertise may contribute code for importing this model into,  another with kernel expertise separately implements a kernel such as batched matrix multiplication and then optimizes it, and another user contributes the code generation, and another the runtime support.

To accomplish this task in today’s ML frameworks a user must first design and train a model which accomplishes the task, know how to use framework tools such as XLA, or TorchScript, be able to modify their complex source bases, and write a complex code generator or LLVM backend and finally modify the runtime to execute on the device, if even possible.

Many engineers in this domain believe that human effort is required to obtain competitive performance that is demonstrated by today’s bleeding edge development, my thesis demonstrated this is not the case.

\section{Dynamic Neural Networks}

Relay addresses the challenges laid out in the previous section by using the composition multiple distinct IR dialects, notably an ML-style high-level dialect, and the use of TVM’s compute language as a low-level dialect, in conjunction with carefully designed runtime and system APIs.

The hypothetical story of the user needing to extend the entire stack is derived from a true sequence of events that occured in the development of Relay. A process like this requires diverse expertise that is often non-compositional. It is nearly impossible to find a single individual who has the ability to do all these tasks at a state-of-the-art level.  One user introduced framework support, another added a kernel, another optimized it, and another added compiler and runtime support for a new target.

We were able to do this in a way where each user was able to make these changes independently and then compose them.

Relay is able to provide this by introducing a set of key interfaces, a high-level extensible IR which can be used to encode various IR dialects, for example we can leverage Relay’s type system to provide the illusion of functional operators until a later dialect in which we change to a destination passing stye.

Although this work began in 2017, the introduction of MLIR project (2018), a framework for composing layered dialects with different semantics, is one validation point of Relay’s design. This thesis argues that future compilers will gain a variety of dialects which must mutually interoperate in order to provide optimal performance.

If we attempt to use existing end-to-end tools such as Lift, they provide a single functional unified-semantics. By embracing a split semantics, with the ability to introduce new dialects we provide different views of the program at different points in order to solve each optimization task independently.


\section{Thesis Organization}

My thesis is organized around four pillars, representation, differentiation, optimization, execution,
We first explore related work and background material
  in \ref{ch:related}, we then discuss the design of the Relay intermediate representation in \ref{ch:design}.
We then discuss how to perform automatic differentiation and training support in \ref{ch:ad} and
  \ref{ch:frameworks}.
We then discuss how to optimize the compiler in \ref{ch:optimizations} and \ref{ch:opt_for_non_experts}.
We then discuss how to lower, execute, and compile these programs in
\ref{ch:compiler}, \ref{ch:dynamic}.
Finally we look to \ref{ch:future} to discuss future work and extensions to this work.


% Bibliography

% [1] "We don’t have better algorithms. We just have more data.”

% [2] data and model size the amount of compute is exponentially growing, roughly doubling every 3.4 months (see https://twitter.com/OpenAI/status/1192481690741903360).

% [3] https://arxiv.org/pdf/1911.05289.pdf

% THESES TO LOOK AT:
% https://people.csail.mit.edu/jrk/jrkthesis.pdf
% https://homes.cs.washington.edu/~djg/theses/sampson_thesis.pdf
% https://llvm.org/pubs/2005-05-04-LattnerPHDThesis.pdf
