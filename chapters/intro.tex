\chapter{Introduction}
\label{ch:intro}

Traditionally, machine learning algorithms were carefully engineered by humans
  in order to leverage limited data to obtain sufficient task performance.
During the Big Data era, these hand engineered algorithms were displaced by
  simpler algorithms applied to the abundantly available data~\citep{unreasonable}\footnote{"We don’t have better algorithms. We just have more data.” From "The Unreasonable Effectiveness of Data"}.
In order to scale these algorithms a number of frameworks were
  developed, most notably streaming and graph processing systems such as MapReduce and Spark.
In the early 2010s the emergence of deep learning changed the paradigm again
  replacing simple regular computation over streams with large batched-models
  composed of compute-hungry tensor computations.
The paradigm shift towards deep learning has started a trend of deploying models
  which are applied to more complex data (i.e., full text documents, images, videos, audio)
  process a greater quantity of data (millions of data points),
  and are more computationally and memory expensive to evaluate on a single datum (i.e., terra-flops of compute per sample).

This new generation of machine learning applications demanded a corresponding era of
  innovation in systems.
The first innovation in deep learning systems were
  ``deep learning frameworks"" such as TensorFlow and PyTorch.
These monolithic systems can execute a subset of deep learning models
  efficiently on a subset of the most popular hardware devices.
These systems provide good baseline performance, but often fail
  when straying from well-used models, and only support limited deployment strategies.
In particular TensorFlow's design was optimized for the distributed setting
  but provides a sub-optimal experience on a single node\footnote{In fact, I have been told by Google engineers they would jokingly refer to TensorFlow as TensorSlow in the early days.}.

Scaling frameworks to span the scope of all models and targets is a monumental endeavor
  requiring deep expertise often not found in conjunction with the skills of a
  machine learning engineer.
Even with the aid of a machine learning framework a machine learning engineer
  cannot write code that scales across the growing sea of
  models, operators, optimizations, and devices while maintaining state-of-the-art performance.
Frameworks only support a subset of programs, optimizations, and devices
  requiring large teams to extend frameworks for even relatively simple deviations
  from standard models.
Even when a team is able to get a model running observed performance often
  is lacking behind the performance of well trod workloads.
Large companies are able to mitigate this cost with raw capital expenditure
  but this puts smaller organizations and researchers on an unequal footing.
In the years after the introduction of frameworks it has become clear
  that automation is needed to both help smaller groups of engineers
  more effectively develop and deploy deep learning, and allow larger
  organizations to work effectively and efficiently.

From the early days of deep learning framework development researchers realized the
  potential for apply compilation to accelerate neural networks.~\footnote{Theano was applying
  compilation to deep learning as early as 2007.}
Modern frameworks borrowing ideas from the research community
  began to introduce compilers into frameworks.
Deep learning compilers, systems for accelerating the execution deep learning via compilation,
 have made similar tradeoffs as frameworks by narrowly focusing on executing a subset
 currently popular of models on a subset of devices.
Designing a deep learning compiler even for this constrained subset is challenging.
For example,
  a popular state-of-the-art DL compiler,
  Google's XLA, can famously slow down programs on
  CPU instead of speeding them up.
Although non-intuitive this is a general challenge of applying compilation,
  it is not a silver bullet and may not provide uniform speedups across all input programs.
Compilers for deep learning~\citep{xla,jax,glow,tvm_osdi18,myia,fluxjl,lattner2020mlir} are
  a rapidly evolving area explored by both industrial and academic researchers.

Deep learning compilers have been narrowly focused
  on the handling of static, feed-forward neural networks.
Due to this design first-generation compilers have been overfit
  for static model compilation, with strong assumptions of static control-flow,
  static tensor dimensions and no complex data structures.
Specifically, these models are represented as static data flow graphs where the
  size of each input and output (i.e. tensors or $n$-dimensional arrays) are known a priori,
  ensuring the execution path remains unchanged on every invocation.
We refer to models with this static nature as \emph{static models}.
Continued advances in neural networks, especially those in natural language processing,
  have introduced new dynamism in models, such as control flow \citep{lstm, language_model},
  dynamic data structures \citep{tree_lstm, graph_lstm}, and dynamic shapes \citep{devlin2018bert}.
  We refer to models exhibiting these behaviors as {\em dynamic models}.
At the time of their design the deep learning was revolutionizing
  computer vision but had not yet changed areas such as natural language processing (NLP).
Unfortunately the design of frameworks have severely limited the performance
  of programs that fall outside of the well-optimized static footprint.

This lack of support has manifested as series of ad-hoc extensions to
  both frameworks, deep learning runtimes, and compilers.
Many existing approaches to dynamic model optimization apply or
  extend existing deep learning frameworks~\citep{xu2018cavs, gao2018low, yu2018dynamic, jeong2018improving, jeong2019janus, dynet, tf_fold}.
Existing work which builds on frameworks extends the programming model either via
  sophisticated additions~\citep{yu2018dynamic} or significant runtime overhead~\citep{tf_fold, jeong2019janus}.

% Other work~\citep{xu2018cavs, gao2018low, tf_fold} which is focused on optimizing specific types
%   of models is often hard to generalize to new models, or generalize over all models.
% Moreover, approaches which inherit from frameworks rely on third-party kernel libraries
%   such as OpenBLAS~\citep{xianyi2014openblas}, cuDNN~\citep{cudnn}, and MKL-DNN~\citep{mkldnn} to achieve competitive performance.
%  These libraries expose a fixed set of operators for the corresponding hardware,
%   compromising the portability of dynamic models which require a large number of operators with varying data types and shapes.
% Designing a new interface independent of existing frameworks provides a clean programming model
%   but often at the cost of performance, due to dynamic interpretation of the model~\citep{dynet}.

% Deep learning compilers differ from traditional deep learning frameworks by separating execution into a compilation,
%   and runtime phase.
% The compilation phase enables whole-model optimization at the graph level,
%   and workload specific kernel code-generation for multiple hardware platforms.

In particular I have spent the last several years focused on an under served problem:
  the representation,
  optimization,
  differentiation,
  and execution of \emph{dynamic neural networks}.
In this thesis I propose generalizing overspecialized
  compilation techniques applied to static dataflow graphs,
  the predominant programming model of deep learning,
  to fully dynamic neural networks.
These generalizations are powered by a simple insight:
  dynamic neural networks are just programs which manipulate tensors.
The challenge is constructing a representation that captures this generality
  in a principled manner, enabling state-of-the-art performance without limiting the programming model.
We realize a unified interface in the form of Relay: a set of systems and APIs designed
  to compose techniques across the stack to achieve state of the art performance.

Instead of building a single IR and compiler to rule them all\footnote{and in the darkness bind them}
  we carefully split the program optimization problem into a series of
  phases each focused on a specific optimization task.
This approach balances the tension between expressivity, composition
  and performance portability.
Part of my thesis work in graduate school
  was meaningful contributions to the general design and implementation of
  production quality deep learning compilers.
Much of this work now exists in Apache TVM, a deep learning compiler framework.
Relay and our work on dynamic neural networks was merged into Apache TVM and
  has catalyzed a major redesign of TVM, in order to embrace and extend the ideas of this thesis.
Relay is deployed in production at multiple leading companies including
  Amazon, Facebook, and Microsoft and is a critical piece of the technology stack
  at OctoML a company I co-founded around the TVM project.
One notable impact is its use in Amazon Alexa, Amazon's AI assistant
  which executes on a variety of devices most notably ``smart speakers''.
Amazon engineers applied it Alexa’s wake word model, executed each time a user interacts with
  Alexa.

\section{State of the Art}

Popular DL compiler intermediate representations (IRs) offer different tradeoffs
  between expressivity, composability, and portability~\citep{
    tensorflow, pytorch_ad, chainer_learningsys2015, tangent, theano, glow}.
Early frameworks adopted IRs
  specialized for then-state-of-the-art models and/or
  emerging hardware accelerators.
As a result, non-trivial extensions require
  patching or even forking frameworks~\citep{
    tf_fold, tf_lite, tangent, tf_eager, xla, glow, torchscript}.
Such \textit{ad hoc} extensions can improve expressivity
  while maintaining backwards compatibility with existing execution mechanisms.
However, they are difficult to design, reason about, and implement,
  often resulting in modifications that are mutually incompatible.
Furthermore extensions which introduce high-level semantics
  such as control-flow, data structures, or data dependent kernels
  are not explicitly represented by deep learning compiler IRs
  and are hard or impossible to effectively optimize.

Let us consider a hypothetical scenario that exemplifies
  IR design tensions in deep learning compilers.
Suppose a machine learning engineer wants to write
  an Android app that uses sentiment analysis to
  determine the moods of its users.
To maintain privacy, the app must run completely on-device,
  i.e., no work can be offloaded to the cloud.
The engineer decides to use a variant of TreeLSTM,
  a deep learning model that uses a tree structure~\citep{tree_lstm}.
Unfortunately, current frameworks' IRs cannot directly encode trees,
  so she must use a framework extension
  like TensorFlow Fold~\citep{tensorflowfold}.

Suppose that after adapting the model to run on her phone,
  the out-of-the-box performance of her
  model on her particular platform is not satisfactory, requiring her to optimize it.
She chooses to employ \textit{quantization}, an optimization that
  potentially trades accuracy for performance by replacing
  floating-point datatypes with low-precision ones.
Although researchers have developed a variety of quantization
  strategies, each of which makes use of different bit-widths, rounding
  modes, and datatypes, our engineer must use a strategy supported
  by existing frameworks~\citep{gustafson2015end, tf_lite_ops_compat, glow_quant}.
Unfortunately, frameworks only provide support for a small number
  of strategies, and supporting new quantization strategies is non-trivial.
Each combination of operator, datatype, bit-width, and
  platform requires unique operator implementations.
Optimizations like operator fusion exacerbate this combinatorial explosion,
  further increasing the number of unique implementations required.
Furthermore, if a framework doesn't have specific support for
  the target phone model she cannot take advantage of specialized deep learning
  instructions or coprocessors~\citep{apple_neural_engine}.

Deep learning frameworks only supported a limited subset of these flows
  and compilers an even smaller subset.
The critical challenge inhibiting optimization via compilation
  was an insufficiently expressive intermediate representation.
We must be able to internalize the constructs we wish to optimize.
Although many frameworks provide end-users with general, turing
  complete programming models the IRs used to lower then to specific
  backends were extremely limited.
Originally these IRs were designed for lowering a directed-acyclic
  graph of operations to high-level libraries provided by device vendors.
Incrementally adapting these IRs to support a richer programming
  model has introduced a semantic gap between the source and target often
  leading to complex or incomplete compilation processes.
For example TorchScript admits arbitrary Python code into its IR, something
  that can be desugared to CUDA operations without employing full Python to GPU compiler.
TensorFlow makes use of Python to often to staged programming over graph fragments,
  using the outer language as a staged-DSL.
Due to this semantic gap many approaches focused on removing dynamic features in
  order to hang on to a simple static compilation model.

A way to view this work is evolving IR and runtime mechanisms to better
  match the source language users are programming in.
Existing compilers have maintained their current IRs as it is not a simple
  task to adapt domain specific transformations to an enriched IR.
There are challenges around efficient automatic differentiation, optimizations around
  reduced or absent static information such as shape sizes, ..., and executing dynamic
  features such as control flow, function calls, or allocating and manipulating data
  structures.
My thesis applies the simple observation that tensor languages can be transformed, compiled,
  optimized and executed just as we would with traditional programming languages.
The central contribution is a recipe for not only growing the IR but also
  adapting domain specific innovations from the computation graph domain to
  a full programming language.
We can do this by building on ideas from decades of previous work in compilation and previous
    trends in compilers in order to solve this problem for an enriched language.

\section{Dynamic Neural Networks}

In particular this thesis talks about growing compiler support from
  a limited subset of programs to a more general programming model.
There are multiple analogies to be made to historical compilation.
For example early Fortran compilers did not support recursion
  as the compiler statically allocated all activation records.
It required evolving both the compiler, IR, and runtime data structures
  to support new features.
Another analogous process was adapting ideas from traditional compilation
  to dynamic languages such as Smalltalk and Self in the 80s and 90s.
These pushes grew existing compilation techniques to support new features
  and programming paradigms.
My thesis is that the first generation of compilers have not yet
  captured the spectrum of user programs requiring us to build
  a new system inspired by previous advances but requiring
  us to solve new deep learning specific challenges.

The correct representation of deep learning is not clear, most
  existing representations have been organically adapted from
  computation graphs a useful if not simplistic IR that organically
  evolved from AD and ML literature.
A representation must capture concepts like control flow, iteration
  or recursive operations like fold or scan, allocation,
  device placement, scoping, effects, function calls, data structures,
  and shape information.
Previous IRs solve a subset of these problems and often only in partial
  or restricted domains.

Even if we have such a representation there are challenges specific to
 machine learning, we must be able to differentiate the programs.
Deep learning has often relied on backpropagation or simple AD algorithms
  defined over computation graphs.
The lack of support for a richer programming model has lead to numerous
  attempts to define both what is differentiable and how to differentiate
  it as we discuss in RELATED WORK.

Even having solved these problems there is still the final challenge of being
  able to optimize and execute the code on the requisite platforms.
During optimization I need to be able to uniformly optimize at a high-level
  of abstraction scalar code optimizers such as LLVM can't appropriate reason
  about programs at the tensor level requiring tensor specific reasoning.
Once I've done that how do I execute the programs such that I can run
  on a variety of devices with acceptable performance.

% In particular these existing systems all employed ways to remove
%   dynamism from the program *before* compilation.
% The end user perceives frameworks as being able to
%   fully execute dynamic programs as framework limitations can mostly be ignored by end
%   users.
% Unfortunately the lack of deep support for dynamic neural networks,
%   as with many things, the devil is in the details, which this lack
%   of support manifesting as strange inconsistencies, limitations, or lost performance.

In this thesis we explore the four key aspects of compiling dynamic
  neural networks their representation, optimization, differentiation,
  and execution.
We do this by building an enriched compiler framework based on TVM by introducing
  a new representation Relay which increases expressivity to handle the features
  we detailed in this section.
We define a new approach to higher-order, higher-order automatic differentiation
  which is able to handle computing derivatives of this language, and
We further extend this compositional framework for compilation to handle dynamism
  and are able to show state-of-the-art performance across a variety of devices.

\section{Thesis Organization}

My thesis is organized around four pillars, representation, differentiation, optimization, execution,
We first explore related work and background material in Chapter~\ref{ch:related},
  we discuss the design of the Relay intermediate representation in Chapter~\ref{ch:relay}
We then discuss how to perform automatic differentiation and training support in Chapter~\ref{ch:ad}.
We then discuss program optimization in Chapter~\ref{ch:optimizations},
  followed by how to lower, execute, and compile these programs in
  Chapter~\ref{ch:execute}.
Finally we look to future in Chapter~\ref{ch:future}.

% Bibliography

% [2] data and model size the amount of compute is exponentially growing, roughly doubling every 3.4 months (see https://twitter.com/OpenAI/status/1192481690741903360).

% [3] https://arxiv.org/pdf/1911.05289.pdf

% THESES TO LOOK AT:
% https://people.csail.mit.edu/jrk/jrkthesis.pdf
% https://homes.cs.washington.edu/~djg/theses/sampson_thesis.pdf
% https://llvm.org/pubs/2005-05-04-LattnerPHDThesis.pdf



% The state of the art when I began my thesis work were
%   deep learning frameworks that mostly had a single "lego" style design.
% I refer to the ``a lego design'' in deep learning frameworks
%   as the developing layers which clear boundaries which
%   allow mixing and match layers of the stack from different vendors.
% For example deep learning frameworks at the top,
%   layered all the way down to the high-performance,
%   device specific kernel libraries provided by hardware manufacturers.
% When I began work on TVM this looked like MxNet as the framework,
%   NNVM as the graph compiler, and the use of TVM or CuDNN as the
%   kernel library.
% As deep learning frameworks began to adopt compilation
%   the resulting compiler designs were heavily constrained by the
%   design of layered abstractions and existing programming model.
% A good analog is the traditional and popular monolithic kernel design
%   in operating systems which provides rigid abstractions over
%   hardware devices.
% Just as modern operating systems are embracing new abstractions
%   such as kernel bypass in order to enable more flexible and
%   performant approaches, deep learning compilers needed more
%   flexible designs.

% An example of a constraining abstraction is the reliance
%   device specific kernels libraries as the bottom most layer.
% By allowing the lowest level of abstraction to only view a single
%   kernel invocation you limit the ability to optimize as the final
%   program must map directly to existing kernels which when possible
%   may be sub-optimal.
% Practically this has introduced a strict separation between so-called
%   ‘graph-compilers’ and so called ‘kernel libraries’.
% These graph compilers implement a simple DAG of operations
%   which they then map down to kernels.
% The graph compilers are a passable solution, assuming the user can describe their program
%   in a restrictive and awkward language, and only makes use of existing kernels.
% If the user model makes use of the well-optimized subset of kernels and well-optimized devices
%   it can assume passable performance, but with many models this is not that case, with very little recourse for program authors.

% In the kernel domain there has been a large amount of existing work on
%   ensuring individual kernels, or pipelines of them are efficient,
%   but these DLSs are not integrated into a larger programming abstraction.
% There existing systems like TensorRT which solve this problem for a specific device,
%   with a series of caveats.

% This state of affairs works OK as long as we remain within the footprint of static
%   models which map well to this restricted programming model.
% Even in the case of TensorFlow which has richer support for control-flow and
%   looping there are challenges optimizing for a single device scenario due
%   to the representation of these constructs.

% When I began this work if a user wanted to write a complex program involving dynamic shapes,
%   dynamic control, and complex data structures which is jointly optimized there are few
%   solutions which solve the above use case.

% Our hypothesis for building Relay is that in order to provide good general
%   purpose performance while also supporting new emerging model features
%   we would require a brand new design for the compiler.
% For example users would often write a brand new kernel to back operations such
%   as an LSTM.
% As we will see in Chapter~\ref{ch:execute} it is possible to write an LSTM in pure Relay
%   and obtain state-of-the-art performance.
% Many engineers in this domain believe that building state-of-the-art kernels requires
%   human effort to obtain bleeding-edge performance, the rest of this thesis demonstrates
%   this is not the case.
