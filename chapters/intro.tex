\chapter{Introduction}
\label{ch:intro}

Traditionally machine learning algorithms were carefully engineered by humans
  in order to leverage limited data to obtain sufficient task performance.
During the Big Data era, these hand engineered algorithms were displaced by
  simpler algorithms applied to the abundantly available data~\citep{unreasonable}~\footnote{"We don’t have better algorithms. We just have more data.” From "The Unreasonable Effectiveness of Data"}.

In order to scale these algorithms a number of systems were
  designed most notably streaming and graph processing systems such as MapReduce, and Spark.
In the early 2010s the emergence of deep learning changed the paradigm again
  replacing simple regular computation over streams of data with large batched-models
  composed of compute-hungry tensor computations.
This trend towards models which are applied to more complex data (i.e., full text documents, images, videos, audio)
  process a greater quantity of data (millions of data points), and are more computationally and memory expensive
  to evaluate on a single datum (i.e., terra-flops of compute per sample).

This new generation of machine learning applications demanded a corresponding era of
  innovation in systems.
The first innovation in deep learning systems were so called
  ‘deep learning frameworks’ such as TensorFlow and PyTorch.
These monolithic systems can execute a subset of deep learning models
  efficiently on a subset of the available hardware devices.
These rigid systems provide good baseline performance, but often fail
  when straying from well-used models, or limited deployment strategies.
In particular TensorFlow's design was optimized for the distributed case
  but provides a sub-optimal experience on a single node.~\footnote{In fact, for a while Google engineers would jokingly refer to TensorFlow as TensorSlow.}

Scaling frameworks to span the scope of all models and targets is a human endeavor
  requiring deep expertise often not found in conjunction with the skills of a
  machine learning engineer.
A single machine learning engineer cannot write code that scales across the growing
  sea of models, operators, optimizations, and devices while maintaining state-of-the-art
  performance.
From the early days of deep learning framework development researchers realized the
  potential for apply compilation to accelerate neural networks.~\footnote{Theano was applying
  compilation to deep learning as early as 2007.}

Deep learning compilers, systems for accelerating the execution deep learning via compilation,
 have made similar tradeoffs as frameworks focusing on executing a subset of models on a subset of devices.
Designing a deep learning compiler even for this constrained subset is challenging.
For example,
  a popular state-of-the-art DL compiler,
  Google's XLA, can famously slow down a programs on
  CPU instead of speeding them up.
Compilers for deep learning are a rapidly evolving area explored by a
  variety of solutions both industrial and academic.
Part of my thesis work in graduate school
  was meaningful contributions to the design of and implementation of
  production quality deep learning compilers.
Much of this work now exists in Apache TVM, a deep learning compiler framework.

In particular I have spent the last several years focused on an under served problem:
  the representation,
  optimization,
  differentiation,
  and execution of dynamic neural networks.
In this thesis I propose that we can generalize overspecialized
  compilation techniques applied to static dataflow graphs,
  the predominant programming model of deep learning,
  to fully dynamic neural networks.
These generalizations are powered by a simple insight:
  dynamic neural networks are just programs which manipulate tensors.
The challenge is constructing a representation that captures this generality
  in a principled manner, enabling state-of-the-art performance without limiting the programming model.
We realize a unified interface in the form of Relay a set of systems and APIs designed
  to compose together techniques across the stack to achieve state of the art performance.

Instead of building a single IR and compiler to rule them all\footnote{and in the darkness bind them}
  we carefully split the program optimization problem into a series of
  phases each focused on a specific optimization task.
By doing this we are able to balance the tension between expressivity, composition
  and performance portability.
Relay is a part of the TVM open source project, and has catalyzed a major redesign of
  TVM, in order to embrace and extend the ideas of this thesis.
Relay is deployed in production at multiple large companies including
  Amazon, Facebook, and Microsoft and is a critical piece of our technology stack
  at OctoML a company founded around the TVM project.
One notable impact is its use in
  optimizing Alexa’s wake word model, executed each time a user interacts with
  Alexa.

\section{State of the Art}

Popular DL compiler intermediate representations (IRs) offer different tradeoffs
  between expressivity, composability, and portability~\citep{
    tensorflow, pytorch_ad, chainer_learningsys2015, tangent, theano, glow}.
Early frameworks adopted IRs
  specialized for then-state-of-the-art models and/or
  emerging hardware accelerators.
As a result, non-trivial extensions require
  patching or even forking frameworks~\citep{
    tf_fold, tf_lite, tangent, tf_eager, xla, glow, torchscript}.
Such \textit{ad hoc} extensions can improve expressivity
  while maintaining backwards compatibility with existing execution mechanisms.
However, they are difficult to design, reason about, and implement,
  often resulting in modifications that are mutually incompatible.

Let us consider a hypothetical scenario that exemplifies
  IR design tensions in DL compilers.
Suppose a machine learning engineer wants to write
  an Android app that uses sentiment analysis to
  determine the moods of its users.
To maintain privacy, the app must run completely on-device,
  i.e., no work can be offloaded to the cloud.
The engineer decides to use a variant of TreeLSTM,
  a deep learning model that uses a tree structure~\citep{tree_lstm}.
Unfortunately, current frameworks' IRs cannot directly encode trees,
  so she must use a framework extension
  like TensorFlow Fold~\citep{tensorflowfold}.

Suppose that after adapting the model to run on her phone,
  the out-of-the-box performance of her
  model on her particular platform is not satisfactory, requiring her to optimize it.
She chooses to employ \textit{quantization}, an optimization that
  potentially trades accuracy for performance by replacing
  floating-point datatypes with low-precision ones.
Although researchers have developed a variety of quantization
  strategies, each of which makes use of different bit-widths, rounding
  modes, and datatypes, our engineer must use a strategy supported
  by existing frameworks~\citep{gustafson2015end, tf_lite_ops_compat, glow_quant}.
Unfortunately, frameworks only provide support for a small number
  of strategies, and supporting new quantization strategies is non-trivial.
Each combination of operator, datatype, bit-width, and
  platform requires unique operator implementations.
Optimizations like operator fusion exacerbate this combinatorial explosion,
  further increasing the number of unique implementations required.
Furthermore, if a framework doesn't have specific support for
  the target phone model she cannot take advantage of specialized deep learning
  instructions or coprocessors~\citep{apple_neural_engine}.

The state of the art when I began my thesis work were
  deep learning frameworks mostly had a single "lego" style design.
Each technology stack had deep learning frameworks at the top,
  layered all the way down to the high-performance,
  device specific kernel libraries provided by hardware manufacturers.
As deep learning frameworks began to adopt compilation
  the resulting compiler designs were heavily constraint by the
  design of layered abstractions and existing programming model.
A good analog is the traditional and popular monolithic kernel design
  in operating systems which provides rigid abstractions over
  hardware devices.
Just as modern operating systems are embracing new abstractions
  such as kernel bypass in order to enable more flexible and
  performant approaches, deep learning compilers needed more
  flexible designs.

An example of a constraining abstraction is the reliance
  device specific kernels libraries as the bottom most layer.
By allowing the lowest level of abstraction to only view a single
  kernel invocation you limit the ability to optimize as the final
  program must map directly to existing kernels which when possible
  may be sub-optimal.
Practically this has introduced a strict separation between so-called
  ‘graph-compilers’ and so called ‘kernel libraries’.
These graph compilers implement a simple DAG of operations
  which they then map down to kernels.
The graph compilers are a passable solution, assuming the user can describe their program
  in a restrictive and awkward language, and only makes use of existing kernels.
If the user model makes use of the well-optimized subset of kernels and well-optimized devices
  it can assume passable performance, but with many models this is not that case, with very little recourse for program authors.

In the kernel domain there has been a large amount of existing work on
  ensuring individual kernels, or pipelines of them are efficient,
  but these DLSs are not integrated into a larger programming abstraction.
There existing systems like TensorRT which solve this problem for a specific device,
  with a series of caveats.

This state of affairs works OK as long as we remain within the footprint of static
  models which map well to this restricted programming model.
Even in the case of TensorFlow which has richer support for control-flow and
  looping there are challenges optimizing for a single device scenario due
  to the representation of these constructs.

When I began this work if a user wanted to write a complex program involving dynamic shapes,
  dynamic control, and complex data structures which is jointly optimized there are few
  solutions which solve the above use case.

Our hypothesis for building Relay is that in order to provide good general
  purpose performance while also supporting new emerging model features
  we would require a brand new design for the compiler.
For example users would often write a brand new kernel to back operations such
  as an LSTM.
As we will see in Chapter~\ref{ch:execute} it is possible to write an LSTM in pure Relay
  and obtain state-of-the-art performance.
Many engineers in this domain believe that building state-of-the-art kernels requires
  human effort to obtain bleeding-edge performance, the rest of this thesis demonstrates
  this is not the case.

\section{Dynamic Neural Networks}

In particular previous solutions have limited potential optimizations
  and transformations in the presence of dynamic features.
In particular these existing systems all employed ways to remove
  dynamism from the program *before* compilation.
The end user perceives frameworks as being able to
  fully execute dynamic programs as framework limitations can mostly be ignored by end
  users.
Unfortunately the lack of deep support for dynamic neural networks,
  as with many things, the devil is in the details, which this lack
  of support manifesting as strange inconsistencies, limitations, or lost performance.

In this thesis we explore the four key aspects of compiling dynamic
  neural networks their representation, optimization, differentiation,
  and execution.
The hypothetical story of the user needing to extend the entire stack is derived
  from a true sequence of events that occurred in the development of Relay.
A process like this requires diverse expertise that is often non-compositional.
It is nearly impossible to find a single individual who has the ability
  to do all these tasks at a state-of-the-art level.
One user introduced framework support, another added a kernel, another optimized it,
  and another added compiler and runtime support for a new target.
We were able to do this in a way where each user was able to make these
  changes independently and then compose them.
We further extend this compositional framework for compilation to handle dynamism
  and are able to show state-of-the-art performance across a variety of devices.

\section{Thesis Organization}

My thesis is organized around four pillars, representation, differentiation, optimization, execution,
We first explore related work and background material
  in \ref{ch:related}, we then discuss the design of the Relay intermediate representation in \ref{ch:relay}.
We then discuss how to perform automatic differentiation and training support in \ref{ch:ad} and
  \ref{ch:frameworks}.
We then discuss how to optimize the compiler in \ref{ch:optimizations} and \ref{ch:opt_for_non_experts}.
We then discuss how to lower, execute, and compile these programs in
\ref{ch:compiler}, \ref{ch:dynamic}.
Finally we look to \ref{ch:future} to discuss future work and extensions to this work.

% Bibliography

% [2] data and model size the amount of compute is exponentially growing, roughly doubling every 3.4 months (see https://twitter.com/OpenAI/status/1192481690741903360).

% [3] https://arxiv.org/pdf/1911.05289.pdf

% THESES TO LOOK AT:
% https://people.csail.mit.edu/jrk/jrkthesis.pdf
% https://homes.cs.washington.edu/~djg/theses/sampson_thesis.pdf
% https://llvm.org/pubs/2005-05-04-LattnerPHDThesis.pdf
