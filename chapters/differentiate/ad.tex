\chapter{Automatic Differentiation}
\label{ch:ad}

In deep learning merely specifying tensor computations is
  one only one piece of the puzzle, as one must be able
  differentiate networks in order to perform optimization.
Many popular optimization algorithms,
  such as stochastic gradient descent
  rely on the ability to take
  the gradient (directional derivative) of the function with
  respect to some parameters.
Early frameworks provided heavily structured optimization frameworks
  which directly implemented the backpropagation algorithm for training
  deep neural networks.
The backpropagation algorithm combines
  computing the gradient of a loss function defined over the
  output of a network with respect to some parameters, and an
  optimization step which updates each parameter based on its
  gradient.
Modern deep learning frameworks have realized that the
  structured imposed by backpropagation is not fundamental
  and model definitions can be expressed more uniformly.
For example we can break down a deep neural network training
  process into (1) a network definition, a function defined over tensor inputs,
  (2) a loss function, which computes a scalar score from tensor
  inputs, and (3) an update step which modifies each of the parameters using their gra
  gradient with respect to the loss function.
All of these pieces can be written as programs which manipulate tensors,
  except for the gradient which requires transforming
  one computation into another which computes its directional derivative.

Early systems such as Theano demonstrated that automatic
  differentiation, the process of automatically computing gradients,
  enables ML researchers to progress more rapidly
  by deriving the gradient of a network and loss instead hand deriving
  the gradient based on their program and writing a secondary gradient function.
There are many ways to approximate or compute the gradient of a function
  but automatic differentiation is favored due to its precise gradient
  values and runtime efficiency.
Early automatic differentiation work made use of runtime data structures
  which store a trace of all operations invoked and computes the gradient
  dynamically by just applying chain rule.
This approach is very flexible as it allows users to implement a small number
  of primitive gradients and handle a large variety of programs which combine
  these primitives.
Unfortunately this purely runtime based approach means that even if a program
  is known a priori we must dynamically construct and execute its gradient.
The dynamic nature induces both static overhead, but also limits ahead of time
  optimization, or deployment to resource limited devices.

In order to address these weaknesses many have reformulated
  automatic differentiation as a source code transformation
  where AD transforms a function which computes its gradient.
The generated gradient function is just a standard tensor programming
  enabling uniform optimization and compilation.
This approach has enjoyed recent popularity due its conceptual
  elegance and uniformity.
For example you have a quantization technique it can be applied
  to both the forward and backward passes of your
  network without needing a special version of quantized AD.
Unfortunately these source code transformations do not cleanly
  generalize to all language features or dynamic behaviors such
  as closures, mutation, control-flow, data structures or dynamic allocations.
There have been numerous previous approaches attempting to handle a large number
  of features as described in Chapter~\ref{ch:related}.

The rest of this chapter explores how we can take the simplicity of
  tape-based AD which can handles higher-order imperative languages
  and convert it into a source code transformation which can handle all
  dynamic features without giving up the ability to compile and optimize.
We close the chapter focusing on how partial evaluation is essential to this
  approach.

\subsection{Automatic-Differentiation}

As discussed in Chapter~\ref{ch:related} Automatic Differentiation has a long history
  and numerous applications across many fields.
The style of automatic differentiation popular when we began this work was the
  what is known as a Wengert-list, a runtime data structure which tracks
  all operations performed, as well as its inputs and outputs, recording them on tape
  that can be later replayed to compute the partial derivative with respect to each variable.
Approaching automatic differentiation in this style leads to a relatively simple
  implementation which can handle arbitrary language features.
When performing differentiation we need not consider data
  structures, control, scope, or any other language features as they
  have been computed away.
We only see a trace of operations of unrolled in time, allowing us
  to simply apply the chain rule from basic calculus.
This approach is beautifully simple as we do not need to formulate
  clever ways to account for each language feature.
Unfortunately the dynamic nature of this solution requires tracking extra runtime state
  and performing computation for each operation, as well as providing an incomplete
  view on the computation which often limits available optimizations.

A design goal of Relay is to provide a target agnostic substrate on which
  to build more complex compilers.
When starting to work on training and automatic differentiation we had
  the same design goal, we wanted to enable training frameworks which
  could use a single code base to provide state-of-the-art performance
  without compromising expressivity.
For example train a model on a cloud GPU or a micro-controller with
  the same code.
To do so we needed an automatic differentiation method that can run
  in multiple execution environments, and handle all of Relay's increased
  expressivity.

We can work backwards from our requirements to eliminate some courses
  of action.
We need an algorithm which handles Relay's dynamic features, maintains
  key properties such as static typing, enables ahead of time optimization,
  and be compiled through standard flows.
During the period we did this work there were numerous
  groups working on variations of this problem for numerous systems
  as detailed in Chapter~\ref{ch:related}.
We believe our design uniquely achieves a few tenuous properties:
\begin{itemize}
  \item Handles all language features including closures,
        references, control flow, and data structures.
  \item Maintains static typing of programs, and by extension shape inference.
  \item Can be performed as an ahead of time source code
    transformation allowing program to be uniformly optimized.
\end{itemize}

After many iterations we discovered a technique that achieves these
  three goals with relative simplicity.

\subsection{Higher-Order, Higher-Order Automatic Differentiation}
\label{sec:autodiff}

Previous automatic differentiation (AD) techniques used on
  computation graphs cannot be directly applied to Relay due to new
  language features such as closures, recursion, and control flow.
Furthermore, it is becoming increasingly important to compute not
  only first-order gradients of functions
  but potentially $n$th-order gradients~\citep{neural_ode, darts}.
Relay requires an algorithm that operates as a source code transformation
  and supports higher-order functions and higher-order gradients.
There is a large body of work on performing AD
  of programs.
Several full-length papers have been written about automatic
  differentiation (AD).
This section highlights the important differences
  between our approach and other recent work.
Previous approaches like Lantern~\citep{lantern} (see Chapter~\ref{ch:related})
  define a generic and powerful version of AD.
Lantern uses delimited continuations to implement AD.
Delimited continuations are an elegant solution
  but require language and compiler support,
  as well garbage collection.
Furthermore, delimited continuations are challenging
  to reason about when performing optimization.

Our AD algorithm is conceptually similar to Lantern's,
  with a few key differences.
First, our algorithm is defined as a source code
  transformation.
Given an expression, Relay produces a corresponding
  expression that computes its gradient.
Figure \ref{fig:autodiff_rules} provides a denotation from
  Relay expression to Relay expression that defines our
  AD algorithm.
Second, our algorithm eschews delimited continuations in favor of
  an approach using closures and references.
Relay simply pairs all tensor values with a reference
  that tracks its partial derivative with respect to its
  output.
This form of reverse mode AD is similar to how one
  would implement forward mode AD.
Relay lifts all tensor-typed values to a pair,
  an expression of some tensor type \verb|T| becomes a tuple of \verb|(T, Ref<T>)|
  where the second component contains the sensitivity variable
  needed to compute the partial derivative.
For each gradient function generated, Relay allocates
  a single reference which stores the ``backpropagator,''
  a closure which propagates gradients from the output to the input.
Each subcomputation affecting the gradient updates this closure; when it is
  finally executed, the built-up closure returns the final derivatives with respect to
  to the arguments.

As described in Figure~\ref{fig:autodiff_rules}, only
  computations involving tensors contribute to the gradient.
For example, we support mutability for free because mutation
  does not affect the gradients.
In this sense, our design is simple.
All tracing needed to compute derivatives is done at run time, enabling
  support for higher order functions, higher order gradients,
  data-dependent control flow, and mutability without requiring changes
  to the algorithm.
% TODO: Remove or rectify this sentence, because the current interface to
% autodiff is via an IR pass.
Finally, Relay exposes this transformation as an operator, allowing users
  to compute the gradient of a function \verb|f| simply by writing \verb|grad(f)|.

Many other variants of AD, including algorithms with different
  complexity bounds (e.g., forward-mode AD), exist.
Forward-mode AD is useful for computing the
  Hessian vector product, which is necessary for techniques like differentiable architecture search
  (DARTS)~\citep{darts}.
Because our AD algorithm just another Relay pass,
  it is possible for users to implement and experiment with different
  AD techniques without changing the system.
To this end, we also implemented a  forward-mode AD algorithm using the traditional method of dual
  numbers~\citep{ad_survey}.
Both forward-mode and reverse-mode AD are higher-order and extensible: they
  support closures, abstract data types, control flow, and recursion.
Although we have not investigated
  composing forward and reverse modes,
  it is possible to mix gradient functions
  because they are regular Relay functions.
Because our algorithm enjoys a closure property,
  we can perform AD over the composition of the gradient
  functions.

\input{chapters/differentiate/autodiff_rules}

\subsection{Partial Evaluator}
\label{sec:partial_eval}

\usemintedstyle{borland}
\begin{figure}
  \centering
  % \texttt{\bf Identity Function}
  \judgbox{\texttt{\bf Identity Function}}{}
    \begin{minted}{rust}
fn <s, bt>(%d: Tensor[s, bt]) {
  %d
}
    \end{minted}

  \judgbox{\texttt{\bf Post-AD}}{}

    \begin{minted}{rust}
fn <s, bt>(%d: Tensor[s, bt]) {
  let %x = ref(fn () { () });
  let %x1 = (%d, ref(zeros_like(%d)));
  let %x2 =
    (fn <s, bt>(
      %d1: (Tensor[s, bt],
            ref(Tensor[s, bt]))) {
      %d1
    })(%x1);
  %x2.1 := ones_like(%x2.0);
  let %x3 = read(%x)();
  (%x2.0, (read(%x1.1),))
}
    \end{minted}
  \judgbox{\texttt{\bf Post-PE}}{}

    \begin{minted}{rust}
fn <s, bt>(%d: Tensor[s, bt]) {
  let %x = fn () {
    let %x1 = ();
    %x1
  };
  let %x2 = ref(%x);
  let %x3 = zeros_like(%d);
  let %x4 = ref(%x3);
  let %x5 = (%d, %x4);
  let %x6 =
    fn <s, bt>(
      %d1: (Tensor[s, bt],
            ref(Tensor[s, bt]))) {
      %d1
    };
  let %x7 = ones_like(%d);
  %x4 := %x7;
  let %x8 = ();
  let %x9 = (%x7,);
  let %x10 = (%d, %x9);
  %x10
}
    \end{minted}

  \judgbox{\texttt{\bf Post-DCE}}{}

    \begin{minted}{rust}
fn <s, bt>(%d: Tensor[s, bt]) {
  (%d, (ones_like(%d),))
}
  \end{minted}
  \caption{
    Example of running the compiler pass pipeline for AD on the identity
    function.
    First, we run the base AD pass on the original function (described in Section \ref{sec:autodiff}).
    Then, we run the partial evaluator,
      which primarily optimizes away the reads and calls in \texttt{\%x2} and
      \texttt{\%x3} in post-AD.
    Since it conservatively determines whether a subexpression is effectful,
      it generates many bindings which are dead code.
    At this point, we run the dead code elimination pass to crunch the code back down.
  }
  \label{fig:pe-example}
\end{figure}
\usemintedstyle{default}

In order to handle differentiating the full IR,
  our AD algorithm makes use of closures and references.
However many of the programs are effectively
  first-order and do not require allocating
  references or a backpropagator closure.
It is essential we remove unnecessary uses
  of closures and references as they inhibit
  optimizations like operator fusion.
Previous approaches have used staging to manually
  phase computation, but this requires modifications
  to the language itself.
A partial evaluator (PE) allows the use of high-level abstractions
  without limiting code that \textit{could} in practice be
  compiled to a particular target.
The benefits of partial evaluation do not only extend to code
  generated by AD but for all of Relay.
Relay's partial evaluator works by defining a interpreter
  where the value domain is partially static values.
The partially static domain represents simple values,
  such as constant tensors, as themselves. The representations
  of aggregate values mirror their structure, enabling
  values which are a mixture of static and dynamic.
This makes the partial evaluator more powerful
  than a constant-folding pass.
The appendix presents an implementation of PE.

There are two important features of our partial evaluator:
  managing effectful computations and handling references.
In order to handle effects, we keep the generated
  program in A-normal form to ensure effects are properly ordered
  and to avoid the duplication of effectful computations.
The partial evaluator supports references by
  simulating the store at partial evaluation time.
The explicit store is threaded throughout execution
  and is managed to achieve flow sensitivity.
After evaluation we construct a new program with
  static subcomputations evaluated
  away.
The reconstructed program contains all original
  expressions, as well as evaluated expressions,
  because interleaving dead-code elimination (DCE) is
  non-trivial.
Afterwards, we separately apply DCE.
The result of this entire process is illustrated
  in Figure~\ref{fig:pe-example}.

\subsection{Partial Evaluator}
\label{l}
Existing deep learning IRs have relied on
  a mixture of staging and constant evaluation
  in order to optimize user programs.
Partial evaluation is a generalized form of constant
  evaluation that can reduce partially constant
  programs.
A partial evaluator (PE) allows the use of high-level abstractions
  without limiting code that \textit{could} in practice be
  compiled to a particular target.
Relay is the first compiler to apply partial evaluation
  techniques to deep learning, the
  core approach of which is based on \citep{pe_ref}.
Partial evaluation, when composed with other
  optimizations like fusion, yields a variety
  of useful optimizations without requiring
  a separate implementation of each.
For example, the partial evaluator can be used to perform
  loop unrolling, which then enables further fusion,
  without any additional compiler passes.

Relay's partial evaluator works by defining a interpreter
  where the value domain is partially static values.
The partially static domain represents simple values,
  such as constant tensors, as themselves.
The representations
  of aggregate values mirror their structure; for example,
  tuples become a tuple of partially static values.
The partially static domain represents dynamic values,
  which may not be known until execution time,
  alongside the static values traditionally supported by
  constant evaluators.
Our partial evaluator must solve two important problems:
  managing effectful computations and handling references.
In order to handle effects, the evaluator keeps the generated
  program in A-normal form to ensure effects are properly ordered
  and restrict duplication of effectful computations.
The partial evaluator supports references by
  simulating the store at partial evaluation time.
The explicit store is threaded throughout execution
  and provides a flow-sensitive PE.
Finally, the evaluator constructs a new program with
  the static subcomputations evaluated away.
% The reconstructed program contains all original
%   expressions, as well as evaluated expressions,
%   because interleaving dead-code elimination (DCE) is
%   non-trivial.
% Afterwards, we separately apply DCE.
% The partial evaluator has use cases beyond inference
%   optimizations, separately it enabled the design of an automatic
%   differentiation algorithm that makes use of hard to optimize
%   features such as references of closures, as the partial evaluator
%   can remove unneeded abstraction.

\section{Partial Evaluator}
Existing deep learning IRs have relied on
  a mixture of staging and constant evaluation
  in order to optimize user programs.
Partial evaluation is a generalized form of constant
  evaluation that can reduce partially constant
  programs.
A partial evaluator (PE) allows the use of high-level abstractions
  without limiting code that \textit{could} in practice be
  compiled to a particular target.
Relay is the first compiler to apply partial evaluation
  techniques to deep learning, the
  core approach of which is based on \citep{pe_ref}.
Partial evaluation, when composed with other
  optimizations like fusion, yields a variety
  of useful optimizations without requiring
  a separate implementation of each.
For example, the partial evaluator can be used to perform
  loop unrolling, which then enables further fusion,
  without any additional compiler passes.

Relay's partial evaluator works by defining a interpreter
  where the value domain is partially static values.
The partially static domain represents simple values,
  such as constant tensors, as themselves.
The representations
  of aggregate values mirror their structure; for example,
  tuples become a tuple of partially static values.
The partially static domain represents dynamic values,
  which may not be known until execution time,
  alongside the static values traditionally supported by
  constant evaluators.
Our partial evaluator must solve two important problems:
  managing effectful computations and handling references.
In order to handle effects, the evaluator keeps the generated
  program in A-normal form to ensure effects are properly ordered
  and restrict duplication of effectful computations.
The partial evaluator supports references by
  simulating the store at partial evaluation time.
The explicit store is threaded throughout execution
  and provides a flow-sensitive PE.
Finally, the evaluator constructs a new program with
  the static subcomputations evaluated away.
The reconstructed program contains all original
  expressions, as well as evaluated expressions,
  because interleaving dead-code elimination (DCE) is
  non-trivial.
Afterwards, we separately apply DCE.
The partial evaluator has use cases beyond inference
  optimizations, separately it enabled the design of an automatic
  differentiation algorithm that makes use of hard to optimize
  features such as references of closures, as the partial evaluator
  can remove unneeded abstraction.

  Existing deep learning IRs have relied on
  a mixture of staging and constant evaluation
  in order to optimize user programs.
Partial evaluation is a generalized form of constant
  evaluation that can reduce partially constant
  programs.
A partial evaluator (PE) allows the use of high-level abstractions
  without limiting code that \textit{could} in practice be
  compiled to a particular target.
Relay is the first compiler to apply partial evaluation
  techniques to deep learning, the
  core approach of which is based on \citep{pe_ref}.
Partial evaluation, when composed with other
  optimizations like fusion, yields a variety
  of useful optimizations without requiring
  a separate implementation of each.
For example, the partial evaluator can be used to perform
  loop unrolling, which then enables further fusion,
  without any additional compiler passes.

Relay's partial evaluator works by defining a interpreter
  where the value domain is partially static values.
The partially static domain represents simple values,
  such as constant tensors, as themselves.
The representations
  of aggregate values mirror their structure; for example,
  tuples become a tuple of partially static values.
The partially static domain represents dynamic values,
  which may not be known until execution time,
  alongside the static values traditionally supported by
  constant evaluators.
Our partial evaluator must solve two important problems:
  managing effectful computations and handling references.
In order to handle effects, the evaluator keeps the generated
  program in A-normal form to ensure effects are properly ordered
  and restrict duplication of effectful computations.
The partial evaluator supports references by
  simulating the store at partial evaluation time.
The explicit store is threaded throughout execution
  and provides a flow-sensitive PE.
Finally, the evaluator constructs a new program with
  the static subcomputations evaluated away.
% The reconstructed program contains all original
%   expressions, as well as evaluated expressions,
%   because interleaving dead-code elimination (DCE) is
%   non-trivial.
% Afterwards, we separately apply DCE.
% The partial evaluator has use cases beyond inference
%   optimizations, separately it enabled the design of an automatic
%   differentiation algorithm that makes use of hard to optimize
%   features such as references of closures, as the partial evaluator
%   can remove unneeded abstraction.

% Existing deep learning IRs have relied on
%   a mixture of staging and constant evaluation
%   in order to optimize user programs.
% Partial evaluation is a generalized form of constant
%   evaluation that can reduce partially constant
%   programs.
% A partial evaluator (PE) allows the use of high-level abstractions
%   without limiting code that \textit{could} in practice be
%   compiled to a particular target.
% Relay is the first compiler to apply partial evaluation
%   techniques to deep learning, the
%   core approach of which is based on \citep{pe_ref}.
% Partial evaluation, when composed with other
%   optimizations like fusion, yields a variety
%   of useful optimizations without requiring
%   a separate implementation of each.
% For example, the partial evaluator can be used to perform
%   loop unrolling, which then enables further fusion,
%   without any additional compiler passes.
