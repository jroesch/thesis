\chapter{Automatic Differentiation}
\label{ch:ad}

In deep learning merely specifying tensor computations is
  one only one piece of the puzzle, as one must be able
  differentiate networks in order to perform optimization.
Many popular optimization algorithms,
  such as stochastic gradient descent
  rely on the ability to take
  the gradient (directional derivative) of the function with
  respect to some parameters.
Early frameworks provided heavily structured optimization frameworks
  which directly implemented the backpropagation algorithm for training
  deep neural networks.
The backpropagation algorithm combines
  computing the gradient of a loss function defined over the
  output of a network with respect to some parameters, and an
  optimization step which updates each parameter based on its
  gradient.
Modern deep learning frameworks have realized that the
  structured imposed by backpropagation is not fundamental
  and model definitions can be expressed more uniformly.
For example we can break down a deep neural network training
  process into a network definition, a function defined over tensor inputs,
  a loss function, which computes a scalar score from tensor
  inputs, the gradients of the loss function with respect to each parameter,
  and an update step which modifies the parameters on each iteration.
Furthermore barring differentiation, the above pieces can all be expressed
  in the tensor computations exposed by deep learning frameworks.

Fortunately there is a wide body of literature on automatic differentiation,
  the process of automatically computing gradients given a function,
  enabling the final missing piece.
Early systems such as Theano demonstrated that automatic
  differentiation enables ML researchers to progress more rapidly
  by deriving the gradient of a network instead hand deriving
  the gradient based on their program and writing a secondary gradient function.
There are many ways to approximate or compute the gradient of a function
  but automatic differentiation is favored due to its precise gradient
  result and runtime efficiency.
Furthermore automatic differentiation can be reframed as a source
  code transformation allowing us to just transform the gradient
  into a standard tensor program and optimize it uniformly.
The rest of this chapter explores how we can generalize previous
  work on automatic-differentiation of higher-order imperative programs
  to Relay, as well as cast it as a source code transformation enabling
  novel application of partial evaluation to the optimization of automatic
  differentiation.

\subsection{Automatic-Differentiation}

As discussed in Chapter~\ref{ch:related} Automatic Differentiation has a long history
  and numerous applications across many fields.
The style of automatic differentiation popular when we began this work was the
  what is known as a Wengert-list, a runtime data structure which tracks
  all the operations performed on each variable, this recorded tape can then
  be played to compute the derivative with respect to each variable.
Approaching automatic differentiation in this style leads to a relatively simple
  implementation which can handle arbitrary language features.
Dynamic approaches although straight forward have a few short comings
  including the need to track extra runtime state and computation,
  overload all relevant operations, and most importantly prevents
  ahead of time optimization.

When we began this project we were interested in an approach which
  provides the expressivity of the dynamic approach without
  sacrificing our ability to optimize, or deploy the code.
In 2017 people began to look into applying previously explored
  ideas which cast automatic differentiation as a source code
  transformation.
During the period we did this work there were numerous
  groups working on this problem for various systems
  as detailed in Chapter~\ref{ch:related}.
Relay introduced some new challenges for us, a hypothesis we
  believed is that true dynamic features would come be required
  by models.
Due to this we needed a technique which can handle dynamic
  features at runtime.
Competing work such ass ZZZ, YYYYYYYY< ZZzZ.
Don't solve this problem.

Our design goals can be summed up as:
\begin{itemize}
  \item Handle closures, control flow, and data structures.
  \item Can be performed as an ahead of time source code
    transformation.
  \item Can be optimized to remove unnecessary computation.
\end{itemize}

After many iterations we discovered that we could use a
  technique that achieves these three goals with relative
  simplicity.

\subsection{Higher-Order, Higher-Order Automatic Differentiation}
\label{sec:autodiff}

Previous automatic differentiation (AD) techniques used on
  computation graphs cannot be directly applied to Relay due to new
  language features such as closures, recursion, and control flow.
Furthermore, it is becoming increasingly important to compute not
  only first-order gradients of functions
  but potentially $n$th-order gradients~\citep{neural_ode, darts}.
Relay requires an algorithm that operates as a source code transformation
  and supports higher-order functions and higher-order gradients.
There is a large body of work on performing AD
  of programs.
Several full-length papers have been written about automatic
  differentiation (AD).
This section highlights the important differences
  between our approach and other recent work.
Previous approaches like Lantern~\citep{lantern} (see Chapter~\ref{ch:related})
  define a generic and powerful version of AD.
Lantern uses delimited continuations to implement AD.
Delimited continuations are an elegant solution
  but require language and compiler support,
  as well garbage collection.
Furthermore, delimited continuations are challenging
  to reason about when performing optimization.

Our AD algorithm is conceptually similar to Lantern's,
  with a few key differences.
First, our algorithm is defined as a source code
  transformation.
Given an expression, Relay produces a corresponding
  expression that computes its gradient.
Figure \ref{fig:autodiff_rules} provides a denotation from
  Relay expression to Relay expression that defines our
  AD algorithm.
Second, our algorithm eschews delimited continuations in favor of
  an approach using closures and references.
Relay simply pairs all tensor values with a reference
  that tracks its partial derivative with respect to its
  output.
This form of reverse mode AD is similar to how one
  would implement forward mode AD.
Relay lifts all tensor-typed values to a pair,
  an expression of some tensor type \verb|T| becomes a tuple of \verb|(T, Ref<T>)|
  where the second component contains the sensitivity variable
  needed to compute the partial derivative.
For each gradient function generated, Relay allocates
  a single reference which stores the ``backpropagator,''
  a closure which propagates gradients from the output to the input.
Each subcomputation affecting the gradient updates this closure; when it is
  finally executed, the built-up closure returns the final derivatives with respect to
  to the arguments.

As described in Figure~\ref{fig:autodiff_rules}, only
  computations involving tensors contribute to the gradient.
For example, we support mutability for free because mutation
  does not affect the gradients.
In this sense, our design is simple.
All tracing needed to compute derivatives is done at run time, enabling
  support for higher order functions, higher order gradients,
  data-dependent control flow, and mutability without requiring changes
  to the algorithm.
% TODO: Remove or rectify this sentence, because the current interface to
% autodiff is via an IR pass.
Finally, Relay exposes this transformation as an operator, allowing users
  to compute the gradient of a function \verb|f| simply by writing \verb|grad(f)|.

Many other variants of AD, including algorithms with different
  complexity bounds (e.g., forward-mode AD), exist.
Forward-mode AD is useful for computing the
  Hessian vector product, which is necessary for techniques like differentiable architecture search
  (DARTS)~\citep{darts}.
Because our AD algorithm just another Relay pass,
  it is possible for users to implement and experiment with different
  AD techniques without changing the system.
To this end, we also implemented a  forward-mode AD algorithm using the traditional method of dual
  numbers~\citep{ad_survey}.
Both forward-mode and reverse-mode AD are higher-order and extensible: they
  support closures, abstract data types, control flow, and recursion.
Although we have not investigated
  composing forward and reverse modes,
  it is possible to mix gradient functions
  because they are regular Relay functions.
Because our algorithm enjoys a closure property,
  we can perform AD over the composition of the gradient
  functions.

\input{chapters/differentiate/autodiff_rules}

\subsection{Partial Evaluator}
\label{sec:partial_eval}

\usemintedstyle{borland}
\begin{figure}
  \centering
  % \texttt{\bf Identity Function}
  \judgbox{\texttt{\bf Identity Function}}{}
    \begin{minted}{rust}
fn <s, bt>(%d: Tensor[s, bt]) {
  %d
}
    \end{minted}

  \judgbox{\texttt{\bf Post-AD}}{}

    \begin{minted}{rust}
fn <s, bt>(%d: Tensor[s, bt]) {
  let %x = ref(fn () { () });
  let %x1 = (%d, ref(zeros_like(%d)));
  let %x2 =
    (fn <s, bt>(
      %d1: (Tensor[s, bt],
            ref(Tensor[s, bt]))) {
      %d1
    })(%x1);
  %x2.1 := ones_like(%x2.0);
  let %x3 = read(%x)();
  (%x2.0, (read(%x1.1),))
}
    \end{minted}
  \judgbox{\texttt{\bf Post-PE}}{}

    \begin{minted}{rust}
fn <s, bt>(%d: Tensor[s, bt]) {
  let %x = fn () {
    let %x1 = ();
    %x1
  };
  let %x2 = ref(%x);
  let %x3 = zeros_like(%d);
  let %x4 = ref(%x3);
  let %x5 = (%d, %x4);
  let %x6 =
    fn <s, bt>(
      %d1: (Tensor[s, bt],
            ref(Tensor[s, bt]))) {
      %d1
    };
  let %x7 = ones_like(%d);
  %x4 := %x7;
  let %x8 = ();
  let %x9 = (%x7,);
  let %x10 = (%d, %x9);
  %x10
}
    \end{minted}

  \judgbox{\texttt{\bf Post-DCE}}{}

    \begin{minted}{rust}
fn <s, bt>(%d: Tensor[s, bt]) {
  (%d, (ones_like(%d),))
}
  \end{minted}
  \caption{
    Example of running the compiler pass pipeline for AD on the identity
    function.
    First, we run the base AD pass on the original function (described in Section \ref{sec:autodiff}).
    Then, we run the partial evaluator,
      which primarily optimizes away the reads and calls in \texttt{\%x2} and
      \texttt{\%x3} in post-AD.
    Since it conservatively determines whether a subexpression is effectful,
      it generates many bindings which are dead code.
    At this point, we run the dead code elimination pass to crunch the code back down.
  }
  \label{fig:pe-example}
\end{figure}
\usemintedstyle{default}

In order to handle differentiating the full IR,
  our AD algorithm makes use of closures and references.
However many of the programs are effectively
  first-order and do not require allocating
  references or a backpropagator closure.
It is essential we remove unnecessary uses
  of closures and references as they inhibit
  optimizations like operator fusion.
Previous approaches have used staging to manually
  phase computation, but this requires modifications
  to the language itself.
A partial evaluator (PE) allows the use of high-level abstractions
  without limiting code that \textit{could} in practice be
  compiled to a particular target.
The benefits of partial evaluation do not only extend to code
  generated by AD but for all of Relay.
Relay's partial evaluator works by defining a interpreter
  where the value domain is partially static values.
The partially static domain represents simple values,
  such as constant tensors, as themselves. The representations
  of aggregate values mirror their structure, enabling
  values which are a mixture of static and dynamic.
This makes the partial evaluator more powerful
  than a constant-folding pass.
The appendix presents an implementation of PE.

There are two important features of our partial evaluator:
  managing effectful computations and handling references.
In order to handle effects, we keep the generated
  program in A-normal form to ensure effects are properly ordered
  and to avoid the duplication of effectful computations.
The partial evaluator supports references by
  simulating the store at partial evaluation time.
The explicit store is threaded throughout execution
  and is managed to achieve flow sensitivity.
After evaluation we construct a new program with
  static subcomputations evaluated
  away.
The reconstructed program contains all original
  expressions, as well as evaluated expressions,
  because interleaving dead-code elimination (DCE) is
  non-trivial.
Afterwards, we separately apply DCE.
The result of this entire process is illustrated
  in Figure~\ref{fig:pe-example}.

\subsection{Partial Evaluator}
\label{sec:partial_eval}
Existing deep learning IRs have relied on
  a mixture of staging and constant evaluation
  in order to optimize user programs.
Partial evaluation is a generalized form of constant
  evaluation that can reduce partially constant
  programs.
A partial evaluator (PE) allows the use of high-level abstractions
  without limiting code that \textit{could} in practice be
  compiled to a particular target.
Relay is the first compiler to apply partial evaluation
  techniques to deep learning, the
  core approach of which is based on \citep{pe_ref}.
Partial evaluation, when composed with other
  optimizations like fusion, yields a variety
  of useful optimizations without requiring
  a separate implementation of each.
For example, the partial evaluator can be used to perform
  loop unrolling, which then enables further fusion,
  without any additional compiler passes.

Relay's partial evaluator works by defining a interpreter
  where the value domain is partially static values.
The partially static domain represents simple values,
  such as constant tensors, as themselves.
The representations
  of aggregate values mirror their structure; for example,
  tuples become a tuple of partially static values.
The partially static domain represents dynamic values,
  which may not be known until execution time,
  alongside the static values traditionally supported by
  constant evaluators.
Our partial evaluator must solve two important problems:
  managing effectful computations and handling references.
In order to handle effects, the evaluator keeps the generated
  program in A-normal form to ensure effects are properly ordered
  and restrict duplication of effectful computations.
The partial evaluator supports references by
  simulating the store at partial evaluation time.
The explicit store is threaded throughout execution
  and provides a flow-sensitive PE.
Finally, the evaluator constructs a new program with
  the static subcomputations evaluated away.
% The reconstructed program contains all original
%   expressions, as well as evaluated expressions,
%   because interleaving dead-code elimination (DCE) is
%   non-trivial.
% Afterwards, we separately apply DCE.
% The partial evaluator has use cases beyond inference
%   optimizations, separately it enabled the design of an automatic
%   differentiation algorithm that makes use of hard to optimize
%   features such as references of closures, as the partial evaluator
%   can remove unneeded abstraction.
