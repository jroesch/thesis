\chapter{Automatic Differentiation}
\label{ch:ad}

In deep learning merely specifying tensor computations is
  one only one piece of the puzzle, as one must be able
  differentiate networks in order to perform optimization.
Many popular optimization algorithms,
  such as stochastic gradient descent
  rely on the ability to take
  the gradient of the function with
  respect to some parameters.
Early frameworks provided heavily structured optimization frameworks
  which directly implemented the backpropagation algorithm for training
  deep neural networks.
The backpropagation algorithm combines
  computing the gradient of a loss function defined over the
  output of a network with respect to some parameters, and an
  optimization step which updates each parameter based on its
  gradient.
Modern deep learning frameworks have realized that the
  structured imposed by backpropagation is not fundamental
  and model definitions can be expressed more uniformly.
For example we can break down a deep neural network training
  process into (1) a network definition, a function defined over tensor inputs,
  (2) a loss function, which computes a scalar score from tensor
  inputs, and (3) an update step which modifies each of the parameters using their gra
  gradient with respect to the loss function.
All of these pieces can be written as programs which manipulate tensors,
  except for transforming one computation into another which computes its gradient.

Early systems such as Theano demonstrated that automatic
  differentiation, the process of automatically computing gradients,
  accelerates research in machine learning by removing the need to manually
  derive gradients.
There are many ways to approximate or compute the gradient of a function
  but automatic differentiation is favored due to its precise gradient
  values and runtime efficiency.
Early automatic differentiation work made use of runtime data structures
  which store a trace of all operations invoked and computes the gradient
  dynamically by just applying chain rule.
This approach is very flexible as it allows users to implement a small number
  of primitive gradients and handle a large variety of programs which combine
  these primitives.
Unfortunately this purely runtime based approach means that even if a program
  is known a priori we must dynamically construct and execute its gradient.
The dynamic nature induces both static overhead, but also limits ahead of time
  optimization, or deployment to resource limited devices.

In order to address these weaknesses many have reformulated
  automatic differentiation as a source code transformation
  where AD transforms a function into one which computes its gradient.
The generated gradient function is just a standard tensor programming
  enabling uniform optimization and compilation.
This approach has enjoyed recent popularity due its conceptual
  elegance and uniformity.
For example a quantization technique defined as a program
  transform can then be applied to the forward and backward passes
  of your network without needing a special version of quantized AD.
Unfortunately these source code transformations do not cleanly
  generalize to all language features or dynamic behaviors such
  as closures, mutation, control-flow, data structures or dynamic allocations.
There have been numerous previous approaches attempting to handle a large number
  of features as described in Chapter~\ref{ch:related}.
The rest of this chapter explores how to build on the simplicity of
  tape-based AD, by adapting it higher-order imperative languages,
  resulting in a source code transformation which supports dynamic features
  without giving up the ability to compile and optimize.
We close the chapter focusing on how partial evaluation is essential to this
  approach.

\subsection{Automatic-Differentiation}

As discussed in Chapter~\ref{ch:related} Automatic Differentiation has a long history
  and numerous applications across many fields.
The style of automatic differentiation popular when we began this work was the
  what is known as a Wengert-list, a runtime data structure which tracks
  all operations performed, as well as its inputs and outputs, recording them on tape
  that can be later replayed to compute the partial derivative with respect to each variable.
Approaching automatic differentiation in this style leads to a relatively simple
  implementation which can handle arbitrary language features.
When performing differentiation we need not consider data
  structures, control, scope, or any other language features as they
  have been computed away.
We only see a trace of operations of unrolled in time, allowing us
  to simply apply the chain rule from basic calculus.
This approach is beautifully simple as we do not need to formulate
  clever ways to account for each language feature.
Unfortunately the dynamic nature of this solution requires tracking extra runtime state
  and performing computation for each operation, as well as providing an incomplete
  view on the computation which often limits available optimizations.

A design goal of Relay is to provide a target agnostic substrate on which
  to build more complex compilers.
When starting to work on training and automatic differentiation we had
  the same design goal, we wanted to enable training frameworks which
  could use a single code base to provide state-of-the-art performance
  without compromising expressivity.
For example train a model on a cloud GPU or a micro-controller without
  needing to change your workflow.
Exploring training demanded an automatic differentiation method that can
  execute in multiple execution environments while handling Relay's increased
  expressivity.
We can work backwards from our requirements to eliminate some courses
  of action.
We need an algorithm which handles Relay's dynamic features, maintains
  key properties such as static typing, enables ahead of time optimization,
  and be compiled through standard flows.
During the period we did this work there were numerous
  groups working on variations of this problem for numerous systems
  as detailed in Chapter~\ref{ch:related}.
We ruled out existing approaches
  as many of them violated our requirements, some required dynamic typing
  and/or reflection, others required complex AD rules per language feature,
  were only defined on SSA, or used staging to remove features not supported
  by less expressive IRs.
After many iterations we discovered a design which uniquely achieved these
  three goals with relative simplicity:
\begin{itemize}
  \item Handles all language features including closures,
        references, control flow, and data structures.
  \item Maintains static typing of programs, and by extension shape inference.
  \item Can be performed as an ahead of time source code
    transformation allowing program to be uniformly optimized.
\end{itemize}

\subsection{Higher-Order, Higher-Order Automatic Differentiation}
\label{sec:autodiff}

Previous automatic differentiation (AD) techniques used on
  computation graphs cannot be directly applied to Relay due to new
  language features such as closures, recursion, and control flow.
Furthermore, it has become increasingly important to compute not
  only first-order gradients of functions
  but potentially $n$th-order gradients~\citep{neural_ode, darts}.
These two challenges were our original motivation to pursue
  a new automatic differentiation technique in Relay.
Our AD algorithm is conceptually a source code transformation
  version of the Wengert List or tape-based method of AD.
We make a few core changes.
First, our algorithm is defined as a source code
  transformation.
Given an expression, Relay produces a corresponding
  expression that computes its gradient.
Figure \ref{fig:autodiff_rules} provides a denotation from
  Relay expression to Relay expression that defines our
  AD algorithm.
Second, our algorithm eschews ideas such as delimited continuations
  in favor of an approach using closures and references.
Our AD algorithm lifts each tensor value into pairs of the original value,
  and a reference which tracks its partial derivative with respect to its
  output.
This form of reverse mode AD is similar to how one
  would implement forward mode AD using dual numbers, except
  we also reverse the program and connect the backwards
  computations via stable references.
Relay lifts all tensor-typed values to a pair,
  an expression of some tensor type \verb|T| becomes a tuple of \verb|(T, Ref<T>)|
  where the second component contains the sensitivity variable
  needed to compute the partial derivative.
For each gradient function generated, Relay allocates
  a single reference which stores the ``backpropagator,''
  a closure which propagates gradients from the output to the input.
Each subcomputation affecting the gradient updates this closure; when it is
  finally executed, the built-up closure returns the final derivatives with respect to
  to the arguments.
As described in Figure~\ref{fig:autodiff_rules}, only
  computations involving tensors contribute to the gradient.
For example, we support mutability for free because mutation
  does not affect the gradients.
In this sense, our design is simple.
All tracing needed to compute derivatives is done at run time, enabling
  support for higher order functions, higher order gradients,
  data-dependent control flow, and mutability without requiring changes
  to the algorithm.
Finally, Relay exposes this transformation as an operator, allowing users
  to compute the gradient of a function \verb|f| simply by writing \verb|grad(f)|.

Many other variants of AD, including algorithms with different
  complexity bounds (e.g., forward-mode AD), exist.
Forward-mode AD is useful for computing the
  Hessian vector product, which is necessary for techniques like differentiable architecture search
  (DARTS)~\citep{darts}.
Because our AD algorithm just another Relay pass,
  it is possible for users to implement and experiment with different
  AD techniques without changing the system.
To this end, we also implemented a  forward-mode AD algorithm using the traditional method of dual
  numbers~\citep{ad_survey}.
Both forward-mode and reverse-mode AD are higher-order and extensible: they
  support closures, abstract data types, control flow, and recursion.
Although we have not investigated
  composing forward and reverse modes,
  it is possible to mix gradient functions
  because they are regular Relay functions.
Because our algorithm enjoys a closure property,
  we can perform AD over the composition of the gradient
  functions.

\input{figures/ad/autodiff_rules}

\subsection{Partial Evaluator}
\label{sec:partial_eval}

\input{figures/ad/pe_example.tex}


Existing deep learning IRs have relied on
  a mixture of staging and constant evaluation
  in order to optimize user programs.
Partial evaluation is a generalized form of constant
  evaluation that can reduce partially constant
  programs.
A partial evaluator (PE) allows the use of high-level abstractions
  without limiting code that \textit{could} in practice be
  compiled to a particular target.
Relay is the first compiler to apply partial evaluation
  techniques to deep learning, the
  core approach of which is based on \citep{pe_ref}.
Partial evaluation, when composed with other
  optimizations like fusion, yields a variety
  of useful optimizations without requiring
  a separate implementation of each.
For example, the partial evaluator can be used to perform
  loop unrolling, which then enables further fusion,
  without any additional compiler passes.

Existing deep learning IRs have relied on
  a mixture of staging and constant evaluation
  in order to optimize user programs.
Partial evaluation is a generalized form of constant
  evaluation that can reduce partially constant
  programs.
A partial evaluator (PE) allows the use of high-level abstractions
  without limiting code that \textit{could} in practice be
  compiled to a particular target.
Relay is the first compiler to apply partial evaluation
  techniques to deep learning, the
  core approach of which is based on \citep{pe_ref}.
Partial evaluation, when composed with other
  optimizations like fusion, yields a variety
  of useful optimizations without requiring
  a separate implementation of each.
For example, the partial evaluator can be used to perform
  loop unrolling, which then enables further fusion,
  without any additional compiler passes.

In order to handle differentiating the full IR,
  our AD algorithm makes use of closures and references.
However many of the programs are effectively
  first-order and do not require allocating
  references or a backpropagator closure.
It is essential we remove unnecessary uses
  of closures and references as they inhibit
  optimizations like operator fusion.
Previous approaches have used staging to manually
  phase computation, but this requires modifications
  to the language itself.
A partial evaluator (PE) allows the use of high-level abstractions
  without limiting code that \textit{could} in practice be
  compiled to a particular target.
The benefits of partial evaluation do not only extend to code
  generated by AD but for all of Relay.
Relay's partial evaluator works by defining a interpreter
  where the value domain is partially static values.
The partially static domain represents simple values,
  such as constant tensors, as themselves.
The representations
  of aggregate values mirror their structure; for example,
  tuples become a tuple of partially static values.
The partially static domain represents dynamic values,
  which may not be known until execution time,
  alongside the static values traditionally supported by
  constant evaluators.
This makes the partial evaluator more powerful
  than a constant-folding pass.
The appendix presents an implementation of PE.

There are two important features of our partial evaluator:
  managing effectful computations and handling references.
In order to handle effects, we keep the generated
  program in A-normal form to ensure effects are properly ordered
  and to avoid the duplication of effectful computations.
The partial evaluator supports references by
  simulating the store at partial evaluation time.
The explicit store is threaded throughout execution
  and is managed to achieve flow sensitivity.
After evaluation we construct a new program with
  static sub-computations evaluated
  away.
The reconstructed program contains all original
  expressions, as well as evaluated expressions,
  because interleaving dead-code elimination (DCE) is
  non-trivial.
Afterwards, we separately apply DCE.
The result of this entire process is illustrated
  in Figure~\ref{fig:pe-example}.
